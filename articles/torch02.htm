<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Torch: framework pro strojové učení i pro zpracování vektorů a tenzorů (2)</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Torch: framework pro strojové učení i pro zpracování vektorů a tenzorů (2)</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>Ve druhé části seriálu o frameworku Torch dokončíme popis metod, které slouží pro manipulaci s&nbsp;komponentami tenzorů i se samotnými tenzory. Budeme tak připraveni na další pokračování, v nichž se již budeme zabývat použitím tenzorů v&nbsp;reálných úlohách.</p>



<h2>Obsah</h2>

<p><a href="#k01">1. Torch: framework pro strojové učení i pro zpracování vektorů a tenzorů (2)</a></p>
<p><a href="#k02">2. Kontinuální uložení komponent tenzorů v&nbsp;operační paměti</a></p>
<p><a href="#k03">3. Operace <strong>select</strong> aneb vytvoření &bdquo;řezu&ldquo; z&nbsp;tenzoru</a></p>
<p><a href="#k04">4. Metoda <strong>select</strong> aplikovaná na 2D matice</a></p>
<p><a href="#k05">5. Metoda <strong>select</strong> aplikovaná na tenzory vyššího řádu</a></p>
<p><a href="#k06">6. Vylepšený operátor indexování</a></p>
<p><a href="#k07">7. Použití operátoru indexování na běžné 1D vektory</a></p>
<p><a href="#k08">8. Použití operátoru indexování pro matice</a></p>
<p><a href="#k09">9. Operátor indexování a tenzory vyšších řádů</a></p>
<p><a href="#k10">10. Získání základních informací o tenzoru &ndash; velikost, počet dimenzí, krok mezi řádky atd.</a></p>
<p><a href="#k11">11. Změna typu komponent tenzoru</a></p>
<p><a href="#k12">12. Změna tvaru či velikosti tenzoru (reshape)</a></p>
<p><a href="#k13">13. Změna tvaru tenzoru a metoda <strong>isContiguous</strong></a></p>
<p><a href="#k14">14. Změna tvaru tenzoru a metody <strong>size</strong>, <strong>dim</strong> a <strong>stride</strong></a></p>
<p><a href="#k15">15. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k16">16. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Torch: framework pro strojové učení i pro zpracování vektorů a tenzorů (2)</h2>

<p><a
href="https://www.root.cz/clanky/torch-framework-pro-strojove-uceni-i-pro-zpracovani-vektoru-a-tenzoru/">V&nbsp;úvodní
části</a> začínajícího seriálu o frameworku <i>Torch</i> jsme si
mj.&nbsp;řekli, že základní datovou strukturou, s&nbsp;níž se v&nbsp;tomto
frameworku pracuje, jsou tenzory. Vektory a matice přitom můžeme
v&nbsp;kontextu Torche považovat za speciální případ tenzorů, takže pro ně
nebylo nutné navrhovat specializované metody. Dnes si popíšeme zejména operaci
<strong>select</strong>, použití operátorů pro indexování (které vás možná
překvapí svými vlastnostmi) a samozřejmě nezapomeneme ani na další operace,
především na operaci určenou pro změnu tvaru tenzoru. Taktéž se seznámíme
s&nbsp;tím, jak lze o tenzorech získat různé užitečné informace.
V&nbsp;navazujícím článku si pak mj.&nbsp;popíšeme i dvě zbývající operace
&ndash; <i>gather</i> a <i>scatter</i>, které umožňují provádění mnohdy velmi
komplikovaných operací s&nbsp;komponentami tenzorů.</p>

<p>Při zkoušení dále popisovaných příkladů použijte buď interaktivní prostředí
Torche:</p>

<pre>
$ <strong>th</strong>
&nbsp;
  ______             __   |  Torch7
 /_  __/__  ________/ /   |  Scientific computing for Lua.
  / / / _ \/ __/ __/ _ \  |  Type ? for help
 /_/  \___/_/  \__/_//_/  |  https://github.com/torch
                          |  http://torch.ch

th&gt;
</pre>

<p>nebo si alternativně naklonujte <a
href="https://github.com/tisnik/torch-examples.git">repositář se všemi
příklady</a> a poté si je můžete postupně spouštět:</p>

<pre>
$ <strong>th 14_select_3rd_order_tensor.lua</strong>
</pre>



<p><a name="k02"></a></p>
<h2 id="k02">2. Kontinuální uložení komponent tenzorů v&nbsp;operační paměti (2)</h2>

<p><a
href="https://www.root.cz/clanky/torch-framework-pro-strojove-uceni-i-pro-zpracovani-vektoru-a-tenzoru/">V&nbsp;první
části</a> seriálu o knihovně <i>Torch</i> jsme si řekli, že tenzory vytvářené
základními konstruktory, mj.&nbsp;tenzor s&nbsp;nulovými komponentami, tenzor
s&nbsp;komponentami nastavenými na jedničku, vektor vytvořený konstruktorem
<strong>range</strong> či tenzor vytvořený z&nbsp;tabulek (polí) jazyka Lua,
jsou v&nbsp;operační paměti uloženy stejným způsobem, jako klasická pole
v&nbsp;programovacím jazyku C. Ovšem některé dále popsané operace mohou vracet
jen vybrané komponenty, které již v&nbsp;obecných případech nemusí
v&nbsp;operační paměti ležet kontinuálně za sebou. To sice z&nbsp;pohledu
uživatele nepředstavuje větší problém (s&nbsp;výsledkem se stále pracuje jako
s&nbsp;plnohodnotným tenzorem), ovšem některé operace se mohou provádět
pomaleji, zejména u rozsáhlejších tenzorů. Mj.&nbsp;i z&nbsp;tohoto důvodu byla
do knihovny <i>Torch</i> přidána metoda <strong>contiguous()</strong> vracející
buď původní tenzor ve chvíli, kdy je v&nbsp;paměti uložen v&nbsp;jediném
kontinuálním bloku, popř.&nbsp;vracející kopii tenzoru ve chvíli, kdy nebyl
uložen kontinuálně (ovšem kopie již kontinuálně uložena je). Záleží jen na
uživateli, zda a v&nbsp;jakém případě tuto metodu použije.</p>

<p>Pokud pouze potřebujete zjistit, jestli jsou komponenty tenzoru uloženy
v&nbsp;paměti kontinuálně, použijte metodu nazvanou
<strong>isContiguous()</strong> vracející pravdivostní hodnotu
<strong>true</strong> či <strong>false</strong>. Použití této metody bude
popsáno v&nbsp;navazujících kapitolách.</p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Operace <strong>select</strong> aneb vytvoření &bdquo;řezu&ldquo; z&nbsp;tenzoru</h2>

<p>Jednou z&nbsp;velmi užitečných a často vyžadovaných operací, které je možné
s&nbsp;objekty typu <strong>Tensor</strong> provádět, je tvorba takzvaných
&bdquo;odřezků&ldquo; či &bdquo;řezů&ldquo; (<i>slice</i>) provedená metodou
nazvanou <strong>select</strong> (zde se terminologie knihovny <i>Torch</i>
odlišuje od některých dalších podobně zaměřených frameworků). Metodě
<strong>select</strong> se předávají dva parametry &ndash; dimenze (první,
druhá, třetí, ...) a index požadovaného řezu v&nbsp;rámci této dimenze:</p>

<pre>
tensor:select(dimenze, index)
</pre>

<p>Výsledkem zavolání této metody je nový tenzor (interně se jedná o pohled
&ndash; <i>view</i> &ndash; na tenzor původní), který má o jednu dimenzi méně,
než zdrojový tenzor. Z&nbsp;této vlastnosti metody <strong>select</strong>
nepřímo vyplývá i to, že tuto metodu není možné použít na jednorozměrné
vektory, i když by se mohlo zdát, že by výsledkem mohl být skalár (proto u
vektorů použijte namísto metody <strong>select</strong> běžné indexování pomocí
operátoru []).</p>



<p><a name="k04"></a></p>
<h2 id="k04">4. Metoda <strong>select</strong> aplikovaná na 2D matice</h2>

<p>Podívejme se nyní na některé způsoby použití metody <strong>select</strong>.
Nejprve se zaměříme na 2D matice, které jsou nejjednodušší strukturou, na
kterou je možné tuto metodu použít.</p>

<p>Nejdřív vytvoříme matici o rozměrech 5&times;5 prvků a naplníme její prvky
hodnotami od 1 do 25. Postup jsme si již vysvětlili minule:</p>

<pre>
th&gt; <strong>x = torch.Tensor(5,5)</strong>
                                                                      [0.0001s]
th&gt; <strong></strong>
                                                                      [0.0000s]
th&gt; <strong>s = x:storage()</strong>
                                                                      [0.0001s]
th&gt; <strong></strong>
                                                                      [0.0000s]
th&gt; <strong>for i = 1,s:size() do</strong>
..&gt;     <strong>s[i]=i</strong>
..&gt; <strong>end</strong>
                                                                      [0.0002s]
th&gt; <strong>x</strong>
  1   2   3   4   5
  6   7   8   9  10
 11  12  13  14  15
 16  17  18  19  20
 21  22  23  24  25
[torch.DoubleTensor of size 5x5]
&nbsp;
                                                                      [0.0005s]
</pre>

<p>Výběr prvního a posledního řádku se provede jednoduše &ndash; metodě
<strong>select</strong> se v&nbsp;prvním parametru předá jednička (první
dimenze) v&nbsp;parametru druhém pak index řádku. Povšimněte si, že výsledkem
je opět tenzor, tentokrát však jednodimenzionální:</p>

<pre>
th&gt; <strong>x:select(1, 1)</strong>
 1
 2
 3
 4
 5
[torch.DoubleTensor of size 5]
&nbsp;
                                                                      [0.0003s]
th&gt; <strong>x:select(1, 5)</strong>
 21
 22
 23
 24
 25
[torch.DoubleTensor of size 5]
&nbsp;
                                                                      [0.0003s]

</pre>

<p>Mimochodem &ndash; stejnou operaci můžeme mnohem jednodušeji provést i
prostým operátorem indexování (kterému samozřejmě dávejte přednost, pokud je to
možné):</p>

<pre>
th&gt; <strong>x[1]</strong>
 1
 2
 3
 4
 5
[torch.DoubleTensor of size 5]
&nbsp;
                                                                      [0.0002s]
th&gt; <strong>x[5]</strong>
 21
 22
 23
 24
 25
[torch.DoubleTensor of size 5]
                                                                      [0.0002s]

</pre>

<p>Následuje výběr prvního a posledního sloupce. Nyní použijeme druhou dimenzi;
první parametr metody <strong>select</strong> tedy bude nastaven na 2 a druhým
parametrem bude index sloupce:</p>

<pre>
th&gt; <strong>x:select(2, 1)</strong>
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]
&nbsp;
                                                                      [0.0003s]
th&gt; <strong>x:select(2, 5)</strong>
  5
 10
 15
 20
 25
[torch.DoubleTensor of size 5]
&nbsp;
                                                                      [0.0003s]
</pre>

<p>Ještě otestujme způsob uložení komponent zdrojového tenzoru i tenzorů
výsledných v&nbsp;operační paměti:</p>

<pre>
th&gt; <strong>x:isContiguous()</strong>
true
                                                                      [0.0001s]
th&gt; <strong>x:select(1,1):isContiguous()</strong>
true
                                                                      [0.0001s]
th&gt; <strong>x:select(1,5):isContiguous()</strong>
true
                                                                      [0.0001s]
th&gt; <strong>x:select(2,1):isContiguous()</strong>
false
                                                                      [0.0001s]
th&gt; <strong>x:select(2,5):isContiguous()</strong>
false
                                                                      [0.0001s]
th&gt; <strong>x[1]:isContiguous()</strong>
true
                                                                      [0.0001s]
</pre>

<p>Z&nbsp;výsledků vyplývá, že komponenty 2D tenzorů jsou v&nbsp;operační
paměti uloženy po řádcích, ovšem u tenzorů, které vznikly výběrem po sloupcích,
tomu tak není.</p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Metoda <strong>select</strong> aplikovaná na tenzory vyššího řádu</h2>

<p>Metodu <strong>select</strong> samozřejmě můžeme použít i pro tenzory
vyšších řádů. Ukažme si to na tenzoru s&nbsp;4&times;4&times;4 komponentami,
které postupně nabývají hodnot od 1 do 64. Nejdříve takový tenzor vytvoříme a
inicializujeme jeho komponenty:</p>

<pre>
th&gt; <strong>t = torch.Tensor(4,4,4)</strong>
                                                                      [0.0001s]
th&gt; <strong></strong>
                                                                      [0.0000s]
th&gt; <strong>s = t:storage()</strong>
                                                                      [0.0000s]
th&gt; <strong></strong>
                                                                      [0.0000s]
th&gt; <strong>for i = 1,s:size() do</strong>
..&gt;     <strong>s[i]=i</strong>
..&gt; <strong>end</strong>
                                                                      [0.0001s]
</pre>

<p>Výsledný tenzor bude vypadat následovně (se způsobem výpisu
&bdquo;trojrozměrných&ldquo; struktur jsme se již seznámili minule, výpis si
můžeme představit jako jednotlivé za sebou seřazené listy, z&nbsp;nichž každý
obsahuje matici):</p>

<pre>
th&gt; <strong>t</strong>
(1,.,.) = 
   1   2   3   4
   5   6   7   8
   9  10  11  12
  13  14  15  16
&nbsp;
(2,.,.) = 
  17  18  19  20
  21  22  23  24
  25  26  27  28
  29  30  31  32
&nbsp;
(3,.,.) = 
  33  34  35  36
  37  38  39  40
  41  42  43  44
  45  46  47  48
&nbsp;
(4,.,.) = 
  49  50  51  52
  53  54  55  56
  57  58  59  60
  61  62  63  64
[torch.DoubleTensor of size 4x4x4]

                                                                      [0.0013s]
</pre>

<p>Nyní vybereme nejdříve první a posléze poslední (čtvrtou) matici,
z&nbsp;nichž každá má 4&times;4 prvky. První dimenze v&nbsp;tomto případě
vybírá jednotlivé matice (můžeme si to představit tak, že jednotlivé matice
4&times;4 prvky jsou umístěny za sebou a tvoří 3D krychli):</p>

<pre>
th&gt; <strong>t:select(1, 1)</strong>
  1   2   3   4
  5   6   7   8
  9  10  11  12
 13  14  15  16
[torch.DoubleTensor of size 4x4]
&nbsp;
                                                                      [0.0003s]
th&gt; <strong>t:select(1, 4)</strong>
 49  50  51  52
 53  54  55  56
 57  58  59  60
 61  62  63  64
[torch.DoubleTensor of size 4x4]
&nbsp;
                                                                      [0.0003s]
</pre>

<p>Opět si povšimněte, že stejného výsledku dosáhneme použitím operátoru
indexování:</p>

<pre>
th&gt; <strong>t[1]</strong>
  1   2   3   4
  5   6   7   8
  9  10  11  12
 13  14  15  16
[torch.DoubleTensor of size 4x4]
&nbsp;
                                                                      [0.0003s]
th&gt; <strong>t[4]</strong>
 49  50  51  52
 53  54  55  56
 57  58  59  60
 61  62  63  64
[torch.DoubleTensor of size 4x4]
&nbsp;
                                                                      [0.0003s]
</pre>

<p>Zkusme nyní složitější příklad, v&nbsp;němž nejdříve vybereme komponenty
ležící v&nbsp;horní vrstvě pomyslné 3D krychle a posléze komponenty ležící
naopak v&nbsp;základě té samé krychle:</p>

<pre>
th&gt; <strong>t:select(2, 1)</strong>
  1   2   3   4
 17  18  19  20
 33  34  35  36
 49  50  51  52
[torch.DoubleTensor of size 4x4]
&nbsp;
                                                                      [0.0004s]
th&gt; <strong>t:select(2, 4)</strong>
 13  14  15  16
 29  30  31  32
 45  46  47  48
 61  62  63  64
[torch.DoubleTensor of size 4x4]
&nbsp;
                                                                      [0.0003s]
</pre>

<p>V&nbsp;další dvojici příkladů získáme matice podél třetí osy (dimenze).
První výsledek odpovídá levé straně pomyslné krychle, druhý výsledek pak straně
pravé:</p>

<pre>
th&gt; <strong>t:select(3, 1)</strong>
  1   5   9  13
 17  21  25  29
 33  37  41  45
 49  53  57  61
[torch.DoubleTensor of size 4x4]
&nbsp;
                                                                      [0.0003s]
th&gt; <strong>t:select(3, 4)</strong>
  4   8  12  16
 20  24  28  32
 36  40  44  48
 52  56  60  64
[torch.DoubleTensor of size 4x4]
&nbsp;
                                                                      [0.0004s]
</pre>

<p>Opět otestujme způsob uložení komponent zdrojového tenzoru i tenzorů
výsledných v&nbsp;operační paměti:</p>

<pre>
th&gt; <strong>t:isContiguous()</strong>
true
                                                                      [0.0001s]
th&gt; <strong>t:select(1,1):isContiguous()</strong>
true
                                                                      [0.0001s]
th&gt; <strong>t:select(1,4):isContiguous()</strong>
true
                                                                      [0.0001s]
th&gt; <strong>t:select(2,1):isContiguous()</strong>
false
                                                                      [0.0001s]
th&gt; <strong>t:select(3,1):isContiguous()</strong>
false
                                                                      [0.0001s]
</pre>

<p>Podle očekávání jsou kontinuálně v&nbsp;paměti umístěny prvky matic
4&times;4 prvky získané řezem podél první osy.</p>



<p><a name="k06"></a></p>
<h2 id="k06">6. Vylepšený operátor indexování</h2>

<p><a
href="https://www.root.cz/clanky/torch-framework-pro-strojove-uceni-i-pro-zpracovani-vektoru-a-tenzoru/">Minule</a>
jsme si popsali metody <strong>sub</strong> a <strong>narrow</strong>,
k&nbsp;nimž dnes přibyla metoda nazvaná <strong>select</strong>. Všechny tyto
metody slouží k&nbsp;výběru určitých komponent z&nbsp;původního (zdrojového)
tenzoru, přičemž výběr může být použit pro čtení nebo i pro zápis,
tj.&nbsp;modifikaci původního tenzoru. Ovšem v&nbsp;knihovně <i>Torch</i> je
možné při konstrukci výběru použít i na první pohled zcela
&bdquo;obyčejný&ldquo; operátor indexování, který uživatelům ve skutečnosti
nabízí poměrně široké možnosti použití, jenž v&nbsp;mnoha běžných
programovacích jazycích nenajdeme. Při indexování komponent tenzoru se používá
tato syntaxe:</p>

<pre>
[{ dimenze1, dimenze2, dimenze3, ... }]
</pre>

<p>popř.:</p>

<pre>
[{ {dimenze1s, dimenze1e}, {dimenze2s, dimenze2e}, {dimenze3s, dimenze3e}, ... }]
</pre>

<p>Povšimněte si, že ve druhém případě se za všech okolností používají složené
závorky uvnitř složených závorek, tj.&nbsp;existuje rozdíl mezi zápisem:</p>

<pre>
[{ 1,2 }]
</pre>

<p>a:</p>

<pre>
[{ {1,2} }]
</pre>

<p>i když se v&nbsp;obou případech do složených závorek zapíše dvojice celých
čísel.</p>

<p>Poznámka: právě z&nbsp;tohoto důvodu se za úvodní dvojicí závorek [{ a před
koncovou dvojicí }] používá mezera, která zvýrazní případný vnitřní blok
{}.</p>

<p>Poznámka<sup>2</sup>: zápis {x,y} v&nbsp;jazyce Lua znamená deklaraci
tabulky (pole) se dvěma prvky. Navíc lze indexy vynechat, což se interpretuje
jako celý sloupec, matice atd. (podle dimenze).</p>



<p><a name="k07"></a></p>
<h2 id="k07">7. Použití operátoru indexování na běžné 1D vektory</h2>

<p>Použití operátoru indexování pro běžné jednorozměrné vektory je velmi
jednoduché, což si ostatně ukážeme na dalších příkladech. Nejdříve vytvoříme
desetiprvkový vektor:</p>

<pre>
th&gt; <strong>v1 = torch.range(1, 10)</strong>
                                                                      [0.0001s]
th&gt; <strong>v1</strong>
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
[torch.DoubleTensor of size 10]
&nbsp;
                                                                      [0.0003s]
</pre>

<p>Dále získáme třetí až osmý prvek vektoru (povšimněte si, jak je při slovním
popisu výhodné začínat indexy prvků od jedničky):</p>

<pre>
th&gt; <strong>v1[{ {3,8} }]</strong>
 3
 4
 5
 6
 7
 8
[torch.DoubleTensor of size 6]
&nbsp;
                                                                      [0.0003s]
</pre>

<p>Můžeme získat i jeden jediný prvek, ovšem v&nbsp;knihovně Torch se stále
bude jednat o tenzor, i když jednoprvkový (prvního řádu):</p>

<pre>
th&gt; <strong>v1[{ {5,5} }]</strong>
 5
[torch.DoubleTensor of size 1]
&nbsp;
                                                                      [0.0003s]
</pre>

<p>Ve skutečnosti lze výsledný tenzor získaný operátorem indexování použít na
<i>levé straně</i> přiřazovacího výrazu, což je překvapivě mocná
konstrukce:</p>

<pre>
th&gt; <strong>v1[{ {3,8} }] =0</strong>
                                                                      [0.0002s]
th&gt; <strong>v1</strong>
  1
  2
  0
  0
  0
  0
  0
  0
  9
 10
[torch.DoubleTensor of size 10]
&nbsp;
                                                                      [0.0003s]
</pre>

<p>Na závěr otestujeme, jestli jsou prvky původního vektoru i vektoru získaného
operátorem indexování, v&nbsp;paměti uloženy kontinuálně:</p>

<pre>
th&gt; <strong>v1:isContiguous()</strong>
true
                                                                      [0.0001s]
th&gt; <strong>v1[{ {3,8} }]:isContiguous()</strong>
true
                                                                      [0.0001s]
</pre>

<p>Vidíme, že tomu tak skutečně je.</p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Použití operátoru indexování pro matice</h2>

<p>V&nbsp;této kapitole si ukážeme, jak se použije rozšířený operátor
indexování pro matice. Nejdříve si vytvoříme matici 5&times;5 prvků a naplníme
ji hodnotami od 1 do 25:</p>

<pre>
th&gt; <strong>x = torch.Tensor(5,5)</strong>
                                                                      [0.0001s]
th&gt; <strong></strong>
                                                                      [0.0000s]
th&gt; <strong>s = x:storage()</strong>
                                                                      [0.0001s]
th&gt; <strong></strong>
                                                                      [0.0000s]
th&gt; <strong>for i = 1,s:size() do</strong>
..&gt;     <strong>s[i]=i</strong>
..&gt; <strong>end</strong>
                                                                      [0.0001s]
</pre>

<p>Výsledná matice vypadá takto:</p>

<pre>
th&gt; <strong>x</strong>
  1   2   3   4   5
  6   7   8   9  10
 11  12  13  14  15
 16  17  18  19  20
 21  22  23  24  25
[torch.DoubleTensor of size 5x5]
&nbsp;
                                                                      [0.0005s]
</pre>

<p>Následně z&nbsp;matice získáme druhý až čtvrtý řádek matice. Již víme, že se
matice ukládají po řádcích, takže indexování je v&nbsp;tomto případě
snadné:</p>

<pre>
th&gt; <strong>x[ {{2,4}} ]</strong>
  6   7   8   9  10
 11  12  13  14  15
 16  17  18  19  20
[torch.DoubleTensor of size 3x5]
&nbsp;
                                                                      [0.0003s]
</pre>

<p>Získání podmatice 3&times;3 prvky již vyžaduje nepatrně složitější
zápis:</p>

<pre>
th&gt; <strong>x[ {{2,4}, {3,5}} ]</strong>
  8   9  10
 13  14  15
 18  19  20
[torch.DoubleTensor of size 3x3]
&nbsp;
                                                                      [0.0002s]
</pre>

<p>V&nbsp;dalším výsledku si povšimněte, že se vrátila matice o rozměrech
1&times;1 prvek a nikoli skalár:</p>

<pre>
th&gt; <strong>x[ {{3}, {3}} ]</strong>
 13
[torch.DoubleTensor of size 1x1]
&nbsp;
                                                                      [0.0002s]
</pre>

<p>Opět můžeme operátor indexování použít na levé straně výrazu, tentokrát pro
vynulování podmatice 4&times;4 prvky:</p>

<pre>
th&gt; <strong>x[ {{1,4}, {1,4}} ] = 0</strong>
                                                                      [0.0001s]
th&gt; <strong>x</strong>
  0   0   0   0   5
  0   0   0   0  10
  0   0   0   0  15
  0   0   0   0  20
 21  22  23  24  25
[torch.DoubleTensor of size 5x5]
&nbsp;
                                                                      [0.0004s]
</pre>

<p>Na závěr si ukážeme nastavení celého (druhého) sloupce na samé devítky.
Povšimněte si použití prázdných {} při specifikaci indexu:</p>

<pre>
th&gt; <strong>x[ {{}, {2}} ] = 9</strong>
                                                                      [0.0000s]
th&gt; <strong>x</strong>
  0   9   0   0   5
  0   9   0   0  10
  0   9   0   0  15
  0   9   0   0  20
 21   9  23  24  25
[torch.DoubleTensor of size 5x5]
&nbsp;
                                                                      [0.0001s]
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Operátor indexování a tenzory vyšších řádů</h2>

<p>Výše popsaný vylepšený operátor indexování je samozřejmě možné použít i u
tenzorů vyšších řádů, což si samozřejmě opět vyzkoušíme. Nejprve vytvoříme
tenzor s&nbsp;64 komponentami:</p>

<pre>
th&gt; <strong>t = torch.Tensor(4,4,4)</strong>
                                                                      [0.0001s]
th&gt; <strong></strong>
                                                                      [0.0000s]
th&gt; <strong>s = t:storage()</strong>
                                                                      [0.0001s]
th&gt; <strong></strong>
                                                                      [0.0000s]
th&gt; <strong>for i = 1,s:size() do</strong>
..&gt;     <strong>s[i]=i</strong>
..&gt; <strong>end</strong>
                                                                      [0.0002s]
</pre>

<p>Výsledný tenzor bude vypadat takto:</p>

<pre>
th&gt; <strong>t</strong>
(1,.,.) = 
   1   2   3   4
   5   6   7   8
   9  10  11  12
  13  14  15  16
&nbsp;
(2,.,.) = 
  17  18  19  20
  21  22  23  24
  25  26  27  28
  29  30  31  32
&nbsp;
(3,.,.) = 
  33  34  35  36
  37  38  39  40
  41  42  43  44
  45  46  47  48
&nbsp;
(4,.,.) = 
  49  50  51  52
  53  54  55  56
  57  58  59  60
  61  62  63  64
[torch.DoubleTensor of size 4x4x4]
&nbsp;
                                                                      [0.0008s]
</pre>

<p>Následující příkaz vybere tenzor složený ze dvou matic 4&times;4 prvky
(jedná se o matice s&nbsp;indexy 2 a 3):</p>

<pre>
th&gt; <strong>t[ {{2,3}} ]</strong>
(1,.,.) = 
  17  18  19  20
  21  22  23  24
  25  26  27  28
  29  30  31  32
&nbsp;
(2,.,.) = 
  33  34  35  36
  37  38  39  40
  41  42  43  44
  45  46  47  48
[torch.DoubleTensor of size 2x4x4]
&nbsp;
                                                                      [0.0002s]
</pre>

<p>Další možnosti &ndash; výběr dvou řádku z&nbsp;výše zmíněných matic:</p>

<pre>
th&gt; <strong>t[ {{2,3},{3,4}} ]</strong>
(1,.,.) = 
  25  26  27  28
  29  30  31  32
&nbsp;
(2,.,.) = 
  41  42  43  44
  45  46  47  48
[torch.DoubleTensor of size 2x2x4]
&nbsp;
                                                                      [0.0001s]
</pre>

<p>Výběr sloupce ze zbývajících matic:</p>

<pre>
th&gt; <strong>t[ {{2,3},{3,4},{1}} ]</strong>
(1,.,.) = 
  25
  29
&nbsp;
(2,.,.) = 
  41
  45
[torch.DoubleTensor of size 2x2x1]
&nbsp;
                                                                      [0.0001s]
</pre>

<p>Výběr (zdánlivě) jediné komponenty, ve skutečnosti však tenzoru třetího
řádu:</p>

<pre>
th&gt; <strong>t[ {{1},{2},{3}} ]</strong>
(1,.,.) = 
  7
[torch.DoubleTensor of size 1x1x1]
&nbsp;
                                                                      [0.0001s]
th&gt; <strong>t[ {{4},{3},{2}} ]</strong>
(1,.,.) = 
  58
[torch.DoubleTensor of size 1x1x1]
&nbsp;
                                                                      [0.0001s]
</pre>



<p><a name="k10"></a></p>
<h2 id="k10">10. Získání základních informací o tenzoru &ndash; velikost, počet dimenzí, krok mezi řádky atd.</h2>

<p>Několik metod slouží k&nbsp;získání základních informací o daném tenzoru.
Především lze zjistit velikost tenzoru (<i>size</i>), počet dimenzí
(<i>dim</i>), počet elementů (<i>nElement</i>), offset mezi sousedními
komponentami, řádky, maticemi ...  (<i>stride</i>) a taktéž offset první
komponenty v&nbsp;rámci &bdquo;pohledu&ldquo; (<i>storageOffset</i>). Všechny
tyto informace můžeme vytisknout touto funkcí:</p>

<pre>
function printTensorInfo(tensor)
    print("Size:")
    print(tensor:size())
    print("Elements:")
    print(tensor:nElement())
    print("Dimensions:")
    print(tensor:dim())
    print("Stride:")
    print(tensor:stride())
    print("Storage offset:")
    print(tensor:storageOffset())
end
</pre>

<p>Funkci můžeme otestovat na několika tenzorech různých řádů:</p>

<pre>
v = torch.range(1, 10)
m = torch.Tensor(5, 6)
t3 = torch.Tensor(4, 5, 6)
t4 = torch.Tensor(2, 3, 4, 5)
&nbsp;
printTensorInfo(v)
printTensorInfo(m)
printTensorInfo(t3)
printTensorInfo(t4)
&nbsp;
printTensorInfo(v[{ {5,8} }])
printTensorInfo(m[{ {1,1},{1,1} }])
printTensorInfo(m[{ {2,2},{2,2} }])
</pre>

<p>Výsledky pro jednotlivé tenzory (ty jsou zvýrazněny).</p>

<p>Běžný vektor s&nbsp;deseti prvky:</p>

<pre>
<strong>torch.range(1, 10)</strong>
&nbsp;
Size:
 10
[torch.LongStorage of size 1]
&nbsp;
Elements:
10
Dimensions:
1
Stride:
 1
[torch.LongStorage of size 1]
&nbsp;
Storage offset:
1
</pre>

<p>Matice 5&times;6 prvků:</p>

<pre>
<strong>torch.Tensor(5, 6)</strong>
&nbsp;
Size:
 5
 6
[torch.LongStorage of size 2]
&nbsp;
Elements:
30
Dimensions:
2
Stride:
 6
 1
[torch.LongStorage of size 2]
&nbsp;
Storage offset:
1
</pre>

<p>Povšimněte si, že výsledkem metody <strong>size()</strong> i
<strong>stride</strong> jsou opět tenzory, tentokrát obsahující celá čísla!</p>

<p>3D struktura 4&times;5&times;6 prvků:</p>

<pre>
<strong>torch.Tensor(4, 5, 6)</strong>
&nbsp;
Size:
 4
 5
 6
[torch.LongStorage of size 3]
&nbsp;
Elements:
120
Dimensions:
3
Stride:
 30
  6
  1
[torch.LongStorage of size 3]
&nbsp;
Storage offset:
1
</pre>

<p>Výsledek <i>stride</i> říká, že dva korespondujícími prvky na sebe
položených matic jsou od sebe vzdáleny 30 prvků, zatímco dva prvky umístěné
v&nbsp;jedné matici pod sebou jsou od sebe vzdáleny o délku řádku, tj.&nbsp;o
šest prvků.</p>

<p>Tenzor čtvrtého řádu:</p>

<pre>
<strong>torch.Tensor(2, 3, 4, 5)</strong>
&nbsp;
Size:
 2
 3
 4
 5
[torch.LongStorage of size 4]
&nbsp;
Elements:
120
Dimensions:
4
Stride:
 60
 20
  5
  1
[torch.LongStorage of size 4]
&nbsp;
Storage offset:
1
</pre>

<p>Pohled na čtyřprvkový vektor (povšimněte si hodnoty Storage offset):</p>

<pre>
<strong>v[{ {5,8} }]</strong>
&nbsp;
Size:
 4
[torch.LongStorage of size 1]
&nbsp;
Elements:
4
Dimensions:
1
Stride:
 1
[torch.LongStorage of size 1]
&nbsp;
Storage offset:
5
</pre>

<p>Pohled na submatici 1&times;1 prvek:</p>

<pre>
<strong>m[{ {1,1},{1,1} }]</strong>
&nbsp;
Size:
 1
 1
[torch.LongStorage of size 2]
&nbsp;
Elements:
1
Dimensions:
2
Stride:
 6
 1
[torch.LongStorage of size 2]
&nbsp;
Storage offset:
1
</pre>

<p>Pohled na submatici 1&times;1 prvek (opět se změnil offset v&nbsp;rámci
původní struktury):</p>

<pre>
<strong>m[{ {2,2},{2,2} }]</strong>
Size:
 1
 1
[torch.LongStorage of size 2]
&nbsp;
Elements:
1
Dimensions:
2
Stride:
 6
 1
[torch.LongStorage of size 2]
&nbsp;
Storage offset:
8
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. Změna typu komponent tenzoru</h2>

<p>V&nbsp;některých případech budeme potřebovat změnit datový typ komponent
tenzoru. Většinou je výchozím typem <strong>Double</strong>, ovšem například u
operací typu <i>gather</i> budeme potřebovat, aby komponenty měly typ
<strong>Long</strong> apod. K&nbsp;tomu můžeme použít metodu
<strong>type</strong>, které se ve formě řetězce předá požadovaný typ.</p>

<p>Přetypovávat budeme prvky vektoru s&nbsp;deseti prvky:</p>

<pre>
th&gt; <strong>v = torch.range(-5, 5)</strong>
                                                                      [0.0002s]
th&gt; <strong>v</strong>
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
[torch.DoubleTensor of size 11]
&nbsp;
                                                                      [0.0004s]
</pre>

<p>Vlastní přetypování je možné provést následovně (povšimněte si změny hodnot
prvků):</p>

<pre>
th&gt; <strong>v2 = v:type("torch.ByteTensor")</strong>
                                                                      [0.0002s]
th&gt; <strong>v2</strong>
 251
 252
 253
 254
 255
   0
   1
   2
   3
   4
   5
[torch.ByteTensor of size 11]
&nbsp;
                                                                      [0.0002s]
th&gt; <strong>v3 = v:type("torch.CharTensor")</strong>
                                                                      [0.0001s]
th&gt; <strong>v3</strong>
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
[torch.CharTensor of size 11]
&nbsp;
                                                                      [0.0002s]
th&gt; <strong>v4 = v:type("torch.ShortTensor")</strong>
                                                                      [0.0001s]
th&gt; <strong>v4</strong>
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
[torch.ShortTensor of size 11]
&nbsp;
                                                                      [0.0002s]
th&gt; <strong>v5 = v:type("torch.IntTensor")</strong>

v6 = v:type("torch.FloatTensor")
v6
                                                                      [0.0022s]
th&gt; <strong>v5</strong>
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
[torch.IntTensor of size 11]
&nbsp;
                                                                      [0.0003s]
th&gt; <strong>v6 = v:type("torch.FloatTensor")</strong>
                                                                      [0.0001s]
th&gt; <strong>v6</strong>
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
[torch.FloatTensor of size 11]
&nbsp;
                                                                      [0.0003s]
</pre>

<p>Alternativně je možné použít specializované metody <strong>byte()</strong>,
<strong>char()</strong> atd:</p>

<pre>
th&gt; <strong>v2_ = v:byte()</strong>
                                                                      [0.0001s]
th&gt; <strong>v2_</strong>
 251
 252
 253
 254
 255
   0
   1
   2
   3
   4
   5
[torch.ByteTensor of size 11]
&nbsp;
                                                                      [0.0002s]
th&gt; <strong>v3_ = v:char()</strong>
                                                                      [0.0001s]
th&gt; <strong>v3_</strong>
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
[torch.CharTensor of size 11]
&nbsp;
                                                                      [0.0002s]
th&gt; <strong>v4_ = v:short()</strong>
                                                                      [0.0001s]
th&gt; <strong>v4_</strong>
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
[torch.ShortTensor of size 11]
&nbsp;
                                                                      [0.0002s]
th&gt; <strong>v5_ = v:int()</strong>
                                                                      [0.0001s]
th&gt; <strong>v5_</strong>
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
[torch.IntTensor of size 11]
&nbsp;
                                                                      [0.0002s]
th&gt; <strong>v6_ = v:float()</strong>
                                                                      [0.0001s]
th&gt; <strong>v6_</strong>
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
[torch.FloatTensor of size 11]
&nbsp;
                                                                      [0.0002s]
</pre>



<p><a name="k12"></a></p>
<h2 id="k12">12. Změna tvaru či velikosti tenzoru (reshape)</h2>

<p>Velmi užitečnou operací je změna tvaru a/nebo velikosti tenzoru.
V&nbsp;některých knihovnách a frameworcích se tato operace jmenuje
<i>reshape</i>, v&nbsp;knihovně Torch se však používá jméno <i>resize</i>.
Použití této metody je jednoduché &ndash; předá se jí požadovaná velikost a
tvar tenzoru ve formátu (dim1, dim2, ...). Pozor si musíme dát pouze na to, že
pokud bude mít nový tenzor více komponent, než tenzor původní, nemusí být nově
přidané komponenty správně inicializovány (nemusí být nulové). Opět se
podívejme na demonstrační příklad.  Nejprve si vytvoříme obyčejný jednorozměrný
vektor s&nbsp;24 prvky:</p>

<pre>
v = torch.range(1, 24)
print(v)
</pre>

<p>Vektor vypadá následovně:</p>

<pre>
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
[torch.DoubleTensor of size 24]
</pre>

<p>Nyní z&nbsp;vektoru vytvoříme matici o velikosti 4&times;6 prvků (čtyři
řádky, šest sloupců):</p>

<pre>
m1 = v:resize(4, 6)
print(m1)
</pre>

<p>Výsledek:</p>

<pre>
  1   2   3   4   5   6
  7   8   9  10  11  12
 13  14  15  16  17  18
 19  20  21  22  23  24
[torch.DoubleTensor of size 4x6]
</pre>

<p>Důležitá poznámka: zavoláním metody <strong>resize</strong> ve skutečnosti
změníme tvar původního vektoru, takže se jedná o (částečně) destruktivní
operaci a nikoli o pouhou funkci! Teoreticky můžete psát pouze:</p>

<pre>
v:resize(4, 6)
print(v)
</pre>

<p>i když podle oficiální dokumentace není výsledek zaručen.</p>

<p>Samozřejmě nám nic nebrání ve vytvoření matice o velikosti 6&times;4
prvků:</p>

<pre>
m2 = v:resize(6, 4)
print(m2)
</pre>

<p>S&nbsp;výsledkem:</p>

<pre>
  1   2   3   4
  5   6   7   8
  9  10  11  12
 13  14  15  16
 17  18  19  20
 21  22  23  24
[torch.DoubleTensor of size 6x4]
</pre>

<p>Můžeme vytvořit i tenzor třetího řádu s&nbsp;2&times;3&times;4=24
komponentami:</p>

<pre>
t1 = v:resize(2, 3, 4)
print(t1)
</pre>

<p>S&nbsp;výsledkem:</p>

<pre>
(1,.,.) = 
   1   2   3   4
   5   6   7   8
   9  10  11  12
&nbsp;
(2,.,.) = 
  13  14  15  16
  17  18  19  20
  21  22  23  24
[torch.DoubleTensor of size 2x3x4]
</pre>

<p>Další varianta:</p>

<pre>
t2 = v:resize(3, 2, 4)
print(t2)
</pre>

<p>Výsledek:</p>

<pre>
(1,.,.) = 
   1   2   3   4
   5   6   7   8
&nbsp;
(2,.,.) = 
   9  10  11  12
  13  14  15  16
&nbsp;
(3,.,.) = 
  17  18  19  20
  21  22  23  24
[torch.DoubleTensor of size 3x2x4]
</pre>

<p>A ještě jedna varianta:</p>

<pre>
t3 = v:resize(3, 4, 2)
print(t3)
</pre>

<p>Výsledek:</p>

<pre>
(1,.,.) = 
   1   2
   3   4
   5   6
   7   8
&nbsp;
(2,.,.) = 
   9  10
  11  12
  13  14
  15  16
&nbsp;
(3,.,.) = 
  17  18
  19  20
  21  22
  23  24
[torch.DoubleTensor of size 3x4x2]
</pre>

<p>Tenzory vyšších řádů:</p>

<pre>
t4 = v:resize(2, 2, 2, 3)
print(t4)
</pre>

<pre>
(1,1,.,.) = 
   1   2   3
   4   5   6
&nbsp;
(2,1,.,.) = 
  13  14  15
  16  17  18
&nbsp;
(1,2,.,.) = 
   7   8   9
  10  11  12
&nbsp;
(2,2,.,.) = 
  19  20  21
  22  23  24
[torch.DoubleTensor of size 2x2x2x3]
</pre>

<pre>
t5 = v:resize(3, 2, 2, 2)
print(t5)
</pre>

<pre>
(1,1,.,.) = 
   1   2
   3   4
&nbsp;
(2,1,.,.) = 
   9  10
  11  12
&nbsp;
(3,1,.,.) = 
  17  18
  19  20
&nbsp;
(1,2,.,.) = 
   5   6
   7   8
&nbsp;
(2,2,.,.) = 
  13  14
  15  16
&nbsp;
(3,2,.,.) = 
  21  22
  23  24
[torch.DoubleTensor of size 3x2x2x2]
</pre>



<p><a name="k13"></a></p>
<h2 id="k13">13. Změna tvaru tenzoru a metoda <strong>isContiguous</strong></h2>

<p>Zjištění, zda jsou prvky tenzoru se změněným tvarem v&nbsp;operační paměti
stále umístěny kontinuálně, je snadné, protože můžeme použít již výše popsanou
metodu <strong>isContiguous</strong>:</p>

<pre>
th&gt; <strong>v = torch.range(1, 24)</strong>
                                                                      [0.0002s]
th&gt; <strong>v:isContiguous()</strong>
true
                                                                      [0.0001s]
</pre>

<pre>
th&gt; <strong>m1 = v:resize(4, 6)</strong>
                                                                      [0.0001s]
th&gt; <strong>m1:isContiguous()</strong>
true
                                                                      [0.0001s]
</pre>

<pre>
th&gt; <strong>m2 = v:resize(6, 4)</strong>
                                                                      [0.0000s]
th&gt; <strong>m2:isContiguous()</strong>
true
                                                                      [0.0000s]
</pre>

<pre>
th&gt; <strong>t1 = v:resize(2, 3, 4)</strong>
                                                                      [0.0000s]
th&gt; <strong>t1:isContiguous()</strong>
true
                                                                      [0.0000s]
</pre>

<pre>
th&gt; <strong>t2 = v:resize(3, 2, 4)</strong>
                                                                      [0.0000s]
th&gt; <strong>t2:isContiguous()</strong>
true
                                                                      [0.0000s]
</pre>

<pre>
th&gt; <strong>t3 = v:resize(3, 4, 2)</strong>
                                                                      [0.0000s]
th&gt; <strong>t3:isContiguous()</strong>
true
                                                                      [0.0000s]
</pre>

<pre>
th&gt; <strong>t4 = v:resize(2, 2, 2, 3)</strong>
                                                                      [0.0000s]
th&gt; <strong>t4:isContiguous()</strong>
true
                                                                      [0.0000s]
</pre>

<pre>
th&gt; <strong>t5 = v:resize(3, 2, 2, 2)</strong>
                                                                      [0.0000s]
th&gt; <strong>t5:isContiguous()</strong>
true
                                                                      [0.0000s]
</pre>

<p>Ve skutečnosti se změna tvaru nijak neprojevila na nutnosti reorganizace či
kopie (klonování) komponent v&nbsp;paměti.</p>



<p><a name="k14"></a></p>
<h2 id="k14">14. Změna tvaru tenzoru a metody <strong>size</strong>, <strong>dim</strong> a <strong>stride</strong></h2>

<p>Zajímavé bude zjistit, jak se změnily vlastnosti tenzoru po modifikaci jeho
tvaru. Opět použijeme nám již známou uživatelsky definovanou funkci:</p>

<pre>
function printTensorInfo(tensor)
    print("Size:")
    print(tensor:size())
    print("Elements:")
    print(tensor:nElement())
    print("Dimensions:")
    print(tensor:dim())
    print("Stride:")
    print(tensor:stride())
    print("Storage offset:")
    print(tensor:storageOffset())
end
</pre>

<p>Postupně ji budeme aplikovat na původní jednorozměrný vektor i na tenzory,
které z&nbsp;něho byly vytvořeny:</p>

<p>Informace o původním vektoru nás už ničím nepřekvapí:</p>

<pre>
th&gt; <strong>v = torch.range(1, 24)</strong>
                                                                      [0.0001s]
th&gt; <strong>printTensorInfo(v)</strong>
Size:
 24
[torch.LongStorage of size 1]
&nbsp;
Elements:
24
Dimensions:
1
Stride:
 1
[torch.LongStorage of size 1]
&nbsp;
Storage offset:
1
                                                                      [0.0004s]
</pre>

<p>Co se stane, když z&nbsp;vektoru vytvoříme matici? Získáme tenzor
s&nbsp;naprosto stejnými vlastnostmi, jakoby byl vytvořen přímo nějakým
konstruktorem:</p>

<pre>
th&gt; <strong>m1 = v:resize(4, 6)</strong>
                                                                      [0.0001s]
th&gt; <strong>printTensorInfo(m1)</strong>
Size:
 4
 6
[torch.LongStorage of size 2]
&nbsp;
Elements:
24
Dimensions:
2
Stride:
 6
 1
[torch.LongStorage of size 2]
&nbsp;
Storage offset:
1
                                                                      [0.0003s]
</pre>

<p>Totéž samozřejmě bude platit i pro tenzor čtvrtého řádu
s&nbsp;3&times;2&times;2&times;2=24 komponentami:</p>

<pre>
th&gt; <strong>t5 = v:resize(3, 2, 2, 2)</strong>
                                                                      [0.0001s]
th&gt; <strong>printTensorInfo(t5)</strong>
Size:
 3
 2
 2
 2
[torch.LongStorage of size 4]
&nbsp;
Elements:
24
Dimensions:
4
Stride:
 8
 4
 2
 1
[torch.LongStorage of size 4]
&nbsp;
Storage offset:
1
                                                                      [0.0003s]
</pre>



<p><a name="k15"></a></p>
<h2 id="k15">15. Repositář s&nbsp;demonstračními příklady</h2>

<p>Všechny demonstrační příklady, které jsme si popsali v&nbsp;předchozích
kapitolách, najdete v&nbsp;GIT repositáři dostupném na adrese <a
href="https://github.com/tisnik/torch-examples.git">https://github.com/tisnik/torch-examples.git</a>.
Následují odkazy na zdrojové kódy jednotlivých příkladů:</p>

<table>
<tr><th>Příklad</th><th>Odkaz</th></tr>
<tr><td>13_select_2nd_order_tensor.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/13_select_2nd_order_tensor.lua">https://github.com/tisnik/torch-examples/blob/master/basics/13_select_2nd_order_tensor.lua</a></td></tr>
<tr><td>14_select_3rd_order_tensor.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/14_select_3rd_order_tensor.lua">https://github.com/tisnik/torch-examples/blob/master/basics/14_select_3rd_order_tensor.lua</a></td></tr>
<tr><td>15_vector_indexing.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/15_vector_indexing.lua">https://github.com/tisnik/torch-examples/blob/master/basics/15_vector_indexing.lua</a></td></tr>
<tr><td>16_matrix_indexing.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/16_matrix_indexing.lua">https://github.com/tisnik/torch-examples/blob/master/basics/16_matrix_indexing.lua</a></td></tr>
<tr><td>17_3rd_order_tensor_indexing.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/17_3rd_order_tensor_indexing.lua">https://github.com/tisnik/torch-examples/blob/master/basics/17_3rd_order_tensor_indexing.lua</a></td></tr>
<tr><td>18_3rd_order_tensor_slicing.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/18_3rd_order_tensor_slicing.lua">https://github.com/tisnik/torch-examples/blob/master/basics/18_3rd_order_tensor_slicing.lua</a></td></tr>
<tr><td>19_size_dim_stride.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/19_size_dim_stride.lua">https://github.com/tisnik/torch-examples/blob/master/basics/19_size_dim_stride.lua</a></td></tr>
<tr><td>20_retype_vector.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/20_retype_vector.lua">https://github.com/tisnik/torch-examples/blob/master/basics/20_retype_vector.lua</a></td></tr>
<tr><td>21_resize.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/21_resize.lua">https://github.com/tisnik/torch-examples/blob/master/basics/21_resize.lua</a></td></tr>
<tr><td>22_resize_is_contiguous.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/22_resize_is_contiguous.lua">https://github.com/tisnik/torch-examples/blob/master/basics/22_resize_is_contiguous.lua</a></td></tr>
<tr><td>23_resize_size_dim_stride.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/23_resize_size_dim_stride.lua">https://github.com/tisnik/torch-examples/blob/master/basics/23_resize_size_dim_stride.lua</a></td></tr>
<tr><td>(pro příští článek):</td><td>&nbsp;</td></tr>
<tr><td>24_gather.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/24_gather.lua">https://github.com/tisnik/torch-examples/blob/master/basics/24_gather.lua</a></td></tr>
</table>



<p><a name="k16"></a></p>
<h2 id="k16">16. Odkazy na Internetu</h2>

<ol>

<li>Stránka projektu Torch<br />
<a href="http://torch.ch/">http://torch.ch/</a>
</li>

<li>Torch na GitHubu (několik repositářů)<br />
<a href="https://github.com/torch">https://github.com/torch</a>
</li>

<li>Torch (machine learning), Wikipedia<br />
<a href="https://en.wikipedia.org/wiki/Torch_%28machine_learning%29">https://en.wikipedia.org/wiki/Torch_%28machine_learning%29</a>
</li>

<li>Torch Package Reference Manual<br />
<a href="https://github.com/torch/torch7/blob/master/README.md">https://github.com/torch/torch7/blob/master/README.md</a>
</li>

<li>Torch Cheatsheet<br />
<a href="https://github.com/torch/torch7/wiki/Cheatsheet">https://github.com/torch/torch7/wiki/Cheatsheet</a>
</li>

<li>An Introduction to Tensors<br />
<a href="https://math.stackexchange.com/questions/10282/an-introduction-to-tensors">https://math.stackexchange.com/questions/10282/an-introduction-to-tensors</a>
</li>

<li>Differences between a matrix and a tensor<br />
<a href="https://math.stackexchange.com/questions/412423/differences-between-a-matrix-and-a-tensor">https://math.stackexchange.com/questions/412423/differences-between-a-matrix-and-a-tensor</a>
</li>

<li>Qualitatively, what is the difference between a matrix and a tensor?<br />
<a href="https://math.stackexchange.com/questions/1444412/qualitatively-what-is-the-difference-between-a-matrix-and-a-tensor?">https://math.stackexchange.com/questions/1444412/qualitatively-what-is-the-difference-between-a-matrix-and-a-tensor?</a>
</li>

<li>BLAS (Basic Linear Algebra Subprograms)<br />
<a href="http://www.netlib.org/blas/">http://www.netlib.org/blas/</a>
</li>

<li>Basic Linear Algebra Subprograms (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms</a>
</li>

<li>Comparison of deep learning software<br />
<a href="https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software">https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software</a>
</li>

<li>TensorFlow<br />
<a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>
</li>

<li>Caffe2 (A New Lightweight, Modular, and Scalable Deep Learning Framework)<br />
<a href="https://caffe2.ai/">https://caffe2.ai/</a>
</li>

<li>PyTorch<br />
<a href="http://pytorch.org/">http://pytorch.org/</a>
</li>

<li>Seriál o programovacím jazyku Lua<br />
<a href="http://www.root.cz/serialy/programovaci-jazyk-lua/">http://www.root.cz/serialy/programovaci-jazyk-lua/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (2)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-2/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-2/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (3)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-3/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-3/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (4)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-4/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-4/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (5 - tabulky a pole)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-5-tabulky-a-pole/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-5-tabulky-a-pole/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (6 - překlad programových smyček do mezijazyka LuaJITu)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-6-preklad-programovych-smycek-do-mezijazyka-luajitu/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-6-preklad-programovych-smycek-do-mezijazyka-luajitu/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (7 - dokončení popisu mezijazyka LuaJITu)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-7-dokonceni-popisu-mezijazyka-luajitu/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-7-dokonceni-popisu-mezijazyka-luajitu/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (8 - základní vlastnosti trasovacího JITu)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-8-zakladni-vlastnosti-trasovaciho-jitu/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-8-zakladni-vlastnosti-trasovaciho-jitu/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (9 - další vlastnosti trasovacího JITu)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-9-dalsi-vlastnosti-trasovaciho-jitu/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-9-dalsi-vlastnosti-trasovaciho-jitu/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (10 - JIT překlad do nativního kódu)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-10-jit-preklad-do-nativniho-kodu/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-10-jit-preklad-do-nativniho-kodu/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (11 - JIT překlad do nativního kódu procesorů s architekturami x86 a ARM)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-11-jit-preklad-do-nativniho-kodu-procesoru-s-architekturami-x86-a-arm/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-11-jit-preklad-do-nativniho-kodu-procesoru-s-architekturami-x86-a-arm/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (12 - překlad operací s reálnými čísly)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-12-preklad-operaci-s-realnymi-cisly/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-12-preklad-operaci-s-realnymi-cisly/</a>
</li>

<li>Lua Profiler (GitHub)<br />
<a href="https://github.com/luaforge/luaprofiler">https://github.com/luaforge/luaprofiler</a>
</li>

<li>Lua Profiler (LuaForge)<br />
<a href="http://luaforge.net/projects/luaprofiler/">http://luaforge.net/projects/luaprofiler/</a>
</li>

<li>ctrace<br />
<a href="http://webserver2.tecgraf.puc-rio.br/~lhf/ftp/lua/">http://webserver2.tecgraf.puc-rio.br/~lhf/ftp/lua/</a>
</li>

<li>The Lua VM, on the Web<br />
<a href="https://kripken.github.io/lua.vm.js/lua.vm.js.html">https://kripken.github.io/lua.vm.js/lua.vm.js.html</a>
</li>

<li>Lua.vm.js REPL<br />
<a href="https://kripken.github.io/lua.vm.js/repl.html">https://kripken.github.io/lua.vm.js/repl.html</a>
</li>

<li>lua2js<br />
<a href="https://www.npmjs.com/package/lua2js">https://www.npmjs.com/package/lua2js</a>
</li>

<li>lua2js na GitHubu<br />
<a href="https://github.com/basicer/lua2js-dist">https://github.com/basicer/lua2js-dist</a>
</li>

<li>Lua (programming language)<br />
<a href="http://en.wikipedia.org/wiki/Lua_(programming_language)">http://en.wikipedia.org/wiki/Lua_(programming_language)</a>
</li>

<li>LuaJIT 2.0 SSA IR
<a href="http://wiki.luajit.org/SSA-IR-2.0">http://wiki.luajit.org/SSA-IR-2.0</a>
</li>

<li>The LuaJIT Project<br />
<a href="http://luajit.org/index.html">http://luajit.org/index.html</a>
</li>

<li>LuaJIT FAQ<br />
<a href="http://luajit.org/faq.html">http://luajit.org/faq.html</a>
</li>

<li>LuaJIT Performance Comparison<br />
<a href="http://luajit.org/performance.html">http://luajit.org/performance.html</a>
</li>

<li>LuaJIT 2.0 intellectual property disclosure and research opportunities<br />
<a href="http://article.gmane.org/gmane.comp.lang.lua.general/58908">http://article.gmane.org/gmane.comp.lang.lua.general/58908</a>
</li>

<li>LuaJIT Wiki<br />
<a href="http://wiki.luajit.org/Home">http://wiki.luajit.org/Home</a>
</li>

<li>LuaJIT 2.0 Bytecode Instructions<br />
<a href="http://wiki.luajit.org/Bytecode-2.0">http://wiki.luajit.org/Bytecode-2.0</a>
</li>

<li>Programming in Lua (first edition)<br />
<a href="http://www.lua.org/pil/contents.html">http://www.lua.org/pil/contents.html</a>
</li>

<li>Lua 5.2 sources<br />
<a href="http://www.lua.org/source/5.2/">http://www.lua.org/source/5.2/</a>
</li>

<li>REPL<br />
<a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop</a>
</li>

<li>The LLVM Compiler Infrastructure<br />
<a href="http://llvm.org/ProjectsWithLLVM/">http://llvm.org/ProjectsWithLLVM/</a>
</li>

<li>clang: a C language family frontend for LLVM<br />
<a href="http://clang.llvm.org/">http://clang.llvm.org/</a>
</li>

<li>LLVM Backend ("Fastcomp")<br />
<a href="http://kripken.github.io/emscripten-site/docs/building_from_source/LLVM-Backend.html#llvm-backend">http://kripken.github.io/emscripten-site/docs/building_from_source/LLVM-Backend.html#llvm-backend</a>
</li>

<li>Lambda the Ultimate: Coroutines in Lua,<br />
<a href="http://lambda-the-ultimate.org/node/438">http://lambda-the-ultimate.org/node/438</a>
</li>

<li>Coroutines Tutorial,<br />
<a href="http://lua-users.org/wiki/CoroutinesTutorial">http://lua-users.org/wiki/CoroutinesTutorial</a>
</li>

<li>Lua Coroutines Versus Python Generators,<br />
<a href="http://lua-users.org/wiki/LuaCoroutinesVersusPythonGenerators">http://lua-users.org/wiki/LuaCoroutinesVersusPythonGenerators</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2017</small></p>
</body>
</html>

