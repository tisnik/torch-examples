<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Framework Torch: pokročilejší operace nad vektory a maticemi</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Framework Torch: pokročilejší operace nad vektory a maticemi</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>Ve čtvrté části seriálu o frameworku Torch nejdříve dokončíme popis problematiky tvorby grafů s&nbsp;využitím knihovny Gnuplot a posléze si popíšeme důležité (pokročilejší) operace, které je možné provádět s vektory i maticemi. Kromě operace typu <i>gather</i> se jedná o skalární, vektorový i vnější součin atd.</p>



<h2>Obsah</h2>

<p><a href="#k01">1. Dokončení problematiky tvorby jednoduchých grafů ve frameworku Torch</a></p>
<p><a href="#k02">2. Zobrazení kontur 2D funkce</a></p>
<p><a href="#k03">3. Zobrazení 3D grafu funkce typu [x,y,z]=f(t)</a></p>
<p><a href="#k04">4. Lorenzův atraktor</a></p>
<p><a href="#k05">5. Operace typu <i>gather</i> provedená nad vektorem</a></p>
<p><a href="#k06">6. Operace typu <i>gather</i> provedená nad maticí</a></p>
<p><a href="#k07">7. Operace nad vektory a maticemi, násobení vektorů, lineární interpolace a další operace z&nbsp;BLAS</a></p>
<p><a href="#k08">8. Vynásobení komponent tenzoru (vektoru, matice) skalární hodnotou</a></p>
<p><a href="#k09">9. Vynásobení odpovídajících si komponent tenzoru</a></p>
<p><a href="#k10">10. Skalární součin vektorů</a></p>
<p><a href="#k11">11. Vektorový součin</a></p>
<p><a href="#k12">12. Vnější součin</a></p>
<p><a href="#k13">13. Součin matice a vektoru</a></p>
<p><a href="#k14">14. Součin dvou matic</a></p>
<p><a href="#k15">15. Lineární transformace</a></p>
<p><a href="#k16">16. Příklad aplikace lineární transformace: otáčení bodu či vektoru v&nbsp;ploše</a></p>
<p><a href="#k17">17. Lineární interpolace mezi dvěma tenzory</a></p>
<p><a href="#k18">18. Další operace dostupné v&nbsp;knihovně BLAS</a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Dokončení problematiky tvorby jednoduchých grafů ve frameworku Torch</h2>

<p><a
href="https://www.root.cz/clanky/framework-torch-serializace-a-deserializace-tenzoru-prace-s-grafy/">V&nbsp;předchozí
části</a> <a
href="https://www.root.cz/serialy/torch-framework-pro-strojove-uceni/">seriálu
o frameworku Torch</a> jsme si mj.&nbsp;ukázali, jakým způsobem je možné
vykreslit graf funkce dvou nezávislých proměnných. Připomeňme si, že se pro
tyto účely používá metoda nazvaná <strong>gnuplot.splot</strong> (<i>surface
plot</i>), které se předá (dvourozměrná) matice s&nbsp;hodnotami funkce. Graf
je automaticky upraven takovým způsobem, aby se vykreslila lokální maxima i
minima funkce; navíc se implicitně pod funkcí zobrazuje i kontura
(&bdquo;vrstevnice&ldquo;) s&nbsp;důležitými hodnotami. Celý příklad, který
vykreslí průběh jednoduché funkce dvou nezávislých proměnných, je možné zapsat
následovně:</p>

<pre>
require("gnuplot")
&nbsp;
GRID=40
&nbsp;
v = torch.range(0, GRID*GRID)
v = v - 12
m = v:resize(GRID, GRID)
m = torch.sin(m*4.0*math.pi/90.0/GRID)
&nbsp;
gnuplot.pngfigure("plot7.png")
gnuplot.title("splot")
gnuplot.splot(m)
gnuplot.plotflush()
gnuplot.close()
</pre>

<img src="https://i.iinfo.cz/images/43/torch3-9.png" class="image-308490" alt="&#160;" width="640" height="480" />
<p><i>Obrázek 1: Graf vykreslený předchozím demonstračním příkladem.</i></p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Zobrazení kontur 2D funkce</h2>

<p>V&nbsp;některých případech však nemusí být zobrazení plochy představující
funkci dostatečně názorné; navíc se v&nbsp;3D grafu mohou některé důležité
hodnoty ztratit (představme si funkci s&nbsp;mnoha lokálními minimy a maximy).
V&nbsp;takovém případě lze namísto metody <strong>gnuplot.splot</strong> použít
metodu <strong>gnuplot.imagesc</strong>. Tato metoda zobrazí funkci dvou
nezávislých proměnných odlišně &ndash; formou běžného rastrového obrázku,
v&nbsp;němž barva jednotlivých oblastí odpovídá hodnotě funkce v&nbsp;daném
místě. Nejjednodušší je situace ve chvíli, kdy do druhého parametru metody
<strong>gnuplot.imagesc</strong> předáme řetězec &bdquo;gray&ldquo;. Potom je
rastrový obrázek zobrazen ve stupních šedi, přičemž nejmenší hodnotě odpovídá
černá barva a nejvyšší hodnotě barva bílá:</p>

<img src="https://i.iinfo.cz/images/392/torch4-1.png" class="image-309062" alt="&#160;" width="640" height="480" />
<p><i>Obrázek 2: Funkce dvou nezávislých proměnných zobrazená formou rastrového
obrázku ve stupních šedi.</i></p>

<p>Obrázek číslo 2 byl vykreslen tímto kódem (funkce <strong>ger</strong> bude
popsána <a href="#k12">níže</a>):</p>

<pre>
require("gnuplot")
&nbsp;
x = torch.linspace(-1,1,120)
xy = torch.ger(x,x)
z = torch.sin(xy*4.0*math.pi)
&nbsp;
gnuplot.pngfigure("plot9.png")
gnuplot.imagesc(z, 'gray')
gnuplot.plotflush()
gnuplot.close()
</pre>

<p>Alternativně je možné obrázek obarvit barvami z&nbsp;palety
s&nbsp;gradientními přechody:</p>

<img src="https://i.iinfo.cz/images/392/torch4-2.png" class="image-309063" alt="&#160;" width="640" height="480" />
<p><i>Obrázek 3: Funkce dvou nezávislých proměnných zobrazená formou rastrového
obrázku ve falešných barvách.</i></p>

<p>Třetí obrázek byl vykreslen s&nbsp;využitím tohoto kódu:</p>

<pre>
require("gnuplot")
&nbsp;
x = torch.linspace(-1,1,120)
xy = torch.ger(x,x)
z = torch.sin(xy*4.0*math.pi)
&nbsp;
gnuplot.pngfigure("plot10.png")
gnuplot.imagesc(z, 'color')
gnuplot.plotflush()
gnuplot.close()
</pre>



<p><a name="k03"></a></p>
<h2 id="k03">3. Zobrazení 3D grafu funkce typu [x,y,z]=f(t)</h2>

<p>Poslední typ grafu, s&nbsp;nímž se dnes seznámíme, je trojrozměrný graf,
v&nbsp;němž se typicky zobrazuje funkce typu <i>[x,y,z]=f(t)</i>
popř.&nbsp;složitější funkce <i>[x<sub>n</sub>, y<sub>n</sub>,
z<sub>n</sub>]=f(x<sub>n-1</sub>, y<sub>n-1</sub>, z<sub>n-1</sub>)</i>. O
zobrazení průběhů těchto funkcí se stará metoda
<strong>gnuplot.scatter3</strong>. Po zavolání této metody, které se předá
trojice vektorů představujících souřadnice vykreslovaných bodů, se automaticky
zjistí potřebné rozsahy na všech třech osách, což je dobře patrné ze čtvrtého
screenshotu. Podívejme se tedy, jakým způsobem je možné zobrazit trojrozměrnou
spirálu (<i>helix</i>). Je to snadné, protože se použije funkce
<i>[x,y,z]=[cos(t), sin(y), t]</i>:</p>

<pre>
require("gnuplot")
&nbsp;
t = torch.linspace(-3 * math.pi, 3 * math.pi, 250)
x = t:clone():cos()
y = t:clone():sin()
&nbsp;
gnuplot.pngfigure("plot11.png")
gnuplot.scatter3(x, y, t)
gnuplot.plotflush()
gnuplot.close()
</pre>

<img src="https://i.iinfo.cz/images/392/torch4-3.png" class="image-309064" alt="&#160;" width="640" height="480" />
<p><i>Obrázek 4: Šroubovice (helix) vykreslená demonstračním příkladem.</i></p>



<p><a name="k04"></a></p>
<h2 id="k04">4. Lorenzův atraktor</h2>

<p>Poměrně vděčným příkladem funkce zobrazené v&nbsp;3D prostoru je dynamický
systém s&nbsp;takzvaným <i>podivným atraktorem</i>, který je nazvaný
<i>Lorenzův atraktor</i> podle svého objevitele. Tento systém sestávající ze
tří dynamických rovnic použil Edward Lorenz v&nbsp;roce 1963 při simulaci
vývoje počasí (resp.&nbsp;ve velmi zjednodušeném modelu počasí). Na tomto
systému byla také numericky a analyticky ověřena velká citlivost na počáteční
podmínky (někdy také nazývaná &bdquo;motýlí efekt&ldquo;). Pro upřesnění je
však nutné říci, že při simulaci na počítači vlastně získáme atraktor, jenž je
periodický. Je to z&nbsp;toho důvodu, že pro zobrazení číselných hodnot je
použito konečného počtu bitů, z&nbsp;toho nutně vyplývá, že se po určitém počtu
kroků (který je však obrovský, takže tento jev mnohdy nezaregistrujeme) začne
dráha Lorenzova atraktoru překrývat. V&nbsp;matematicky přesném modelu však
tato situace nenastane, každá smyčka funkce bude mít unikátní tvar a dráhy se
nebudou překrývat, pouze protínat. Diferenciální rovnice Lorenzova atraktoru
mají po převodu na diferenční tvar následující formát:</p>

<p>
dx/dt = &sigma; (y-x)<br />
dy/dt = x(&rho; - z) - y<br />
dz/dt = xy - &Beta;z<br />
</p>

<p>Takže pro iterativní (samozřejmě že nepřesný) výpočet můžeme pracovat
s&nbsp;následujícími vztahy, které pro dostatečně malé <i>dt</i> vedou
k&nbsp;výpočtu bodů tvořících Lorenzův atraktor:</p>

<p>
x<sub>n+1</sub>=x<sub>n</sub>+(&sigma; (y-x)) dt<br />
y<sub>n+1</sub>=y<sub>n</sub>+(x(&rho; - z) - y) dt<br />
z<sub>n+1</sub>=z<sub>n</sub>+(xy - &Beta;z) dt<br />
</p>

<p>Podívejme se nyní na způsob implementace této funkce:</p>

<pre>
<i>-- funkce pro výpočet dalšího bodu Lorenzova atraktoru</i>
function lorenz(x, y, z, s, r, b)
    x_dot = s*(y - x)
    y_dot = r*x - y - x*z
    z_dot = x*y - b*z
    return x_dot, y_dot, z_dot
end
</pre>

<p>A výpočtu sekvence bodů ležících na atraktoru:</p>

<pre>
<i>-- krok (změna času)</i>
dt = 0.01
&nbsp;
<i>-- celkový počet vypočtených bodů na Lorenzově atraktoru</i>
n = 10000
&nbsp;
<i>-- prozatím prázdné vektory připravená pro výpočet</i>
x = torch.zeros(n)
y = torch.zeros(n)
z = torch.zeros(n)
&nbsp;
<i>-- počáteční hodnoty</i>
x[1], y[1], z[1] = 0., 1., 1.05
&nbsp;
<i>-- vlastní výpočet atraktoru</i>
for i=1, n-1 do
    x_dot, y_dot, z_dot = lorenz(x[i], y[i], z[i], 10, 28, 2.667)
    x[i+1] = x[i] + x_dot * dt
    y[i+1] = y[i] + y_dot * dt
    z[i+1] = z[i] + z_dot * dt
end
</pre>

<img src="https://i.iinfo.cz/images/392/torch4-4.png" class="image-309065" alt="&#160;" width="640" height="480" />
<p><i>Obrázek 5: Lorenzův atraktor vykreslený přes framework Torch.</i></p>

<p>Celý příklad i se zobrazením atraktoru lze naprogramovat například
takto:</p>

<pre>
require("gnuplot")
&nbsp;
<i>-- funkce pro výpočet dalšího bodu Lorenzova atraktoru</i>
function lorenz(x, y, z, s, r, b)
    x_dot = s*(y - x)
    y_dot = r*x - y - x*z
    z_dot = x*y - b*z
    return x_dot, y_dot, z_dot
end
&nbsp;
<i>-- krok (změna času)</i>
dt = 0.01
&nbsp;
<i>-- celkový počet vypočtených bodů na Lorenzově atraktoru</i>
n = 10000
&nbsp;
<i>-- prozatím prázdné vektory připravená pro výpočet</i>
x = torch.zeros(n)
y = torch.zeros(n)
z = torch.zeros(n)
&nbsp;
<i>-- počáteční hodnoty</i>
x[1], y[1], z[1] = 0., 1., 1.05
&nbsp;
<i>-- vlastní výpočet atraktoru</i>
for i=1, n-1 do
    x_dot, y_dot, z_dot = lorenz(x[i], y[i], z[i], 10, 28, 2.667)
    x[i+1] = x[i] + x_dot * dt
    y[i+1] = y[i] + y_dot * dt
    z[i+1] = z[i] + z_dot * dt
end
&nbsp;
<i>-- vykreslení grafu</i>
gnuplot.pngfigure("plot12.png")
gnuplot.raw("set pointsize 0.1")
gnuplot.scatter3("lorenz attractor", x, y, z)
gnuplot.plotflush()
gnuplot.close()
</pre>

<p>Poznámka: za povšimnutí též stojí řádek:</p>

<pre>
gnuplot.raw("set pointsize 0.1")
</pre>

<p>který přímo (bez dalšího zpracování) volá příkaz gnuplotu &ndash; samotná
knihovna <i>Torch</i> totiž zdaleka nereflektuje všechny možnosti gnuplotu.</p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Operace typu <i>gather</i> provedená nad vektorem</h2>

<p>Velmi užitečná, i když na první pohled možná poněkud komplikovaná, je
operace typu <i>gather</i> představovaná metodou
<strong>tensor:gather</strong>. Tato operace může být aplikována například na
jednorozměrný vektor; argumentem metody <strong>tensor:gather</strong> bude
další vektor, který musí obsahovat celá čísla (typu int, long atd.).
V&nbsp;takovém případě metoda <strong>tensor:gather</strong> vrátí ty prvky
původního vektoru, které leží na zvolených indexech:</p>

<pre>
v1 = torch.range(10, 100, 10)
print(v1)
&nbsp;
indexes = torch.LongTensor({2, 4, 6, 8, 10})
print(indexes)
&nbsp;
print(v1:gather(1, indexes))
&nbsp;
print(v1:gather(1, indexes):isContiguous())
&nbsp;
print("-----------------------")
&nbsp;
indexes = torch.range(10, 1, -1):long()
print(indexes)
&nbsp;
print(v1:gather(1, indexes))
&nbsp;
print(v1:gather(1, indexes):isContiguous())
</pre>

<p>Podívejme se na zprávy vypisované příkladem. Nejprve se vypíše obsah
původního vektoru:</p>

<pre>
  10
  20
  30
  40
  50
  60
  70
  80
  90
 100
[torch.DoubleTensor of size 10]
</pre>

<p>Dále se vypíše vektor s&nbsp;indexy vybíraných prvků:</p>

<pre>
  2
  4
  6
  8
 10
[torch.LongTensor of size 5]
</pre>

<p>A na konec výsledný vektor s&nbsp;vybranými prvky:</p>

<pre>
  20
  40
  60
  80
 100
[torch.DoubleTensor of size 5]
</pre>

<p>Nový vektor s&nbsp;indexy tentokrát obsahuje hodnoty od 10 do 1:</p>

<pre>
 10
  9
  8
  7
  6
  5
  4
  3
  2
  1
[torch.LongTensor of size 10]
</pre>

<p>Výsledkem operace <i>gather</i> jsou prvky původního vektoru, ovšem
v&nbsp;opačném pořadí:</p>

<pre>
 100
  90
  80
  70
  60
  50
  40
  30
  20
  10
[torch.DoubleTensor of size 10]
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Operace typu <i>gather</i> provedená nad maticí</h2>

<p>Operaci <i>gather</i> lze aplikovat i na matice, její použití je však
v&nbsp;tomto případě složitější, protože indexy nyní nemusí tvořit pouhý
jednorozměrný vektor, ale mohou mít podobu 2D matice s&nbsp;různým počtem řádků
a sloupců. Příklad použití nyní bude komplikovanější, protože si ukážeme různé
možnosti, které nám tato operace nabízí. Nejprve vytvoříme zdrojovou
matici:</p>

<pre>
v = torch.range(1, 24)
m1 = v:resize(6, 4)
print(m1)
</pre>

<pre>
  1   2   3   4
  5   6   7   8
  9  10  11  12
 13  14  15  16
 17  18  19  20
 21  22  23  24
[torch.DoubleTensor of size 6x4]
</pre>

<p>Postupně budeme maticí po sloupcích procházet a vybereme vždy první prvek
sloupce:</p>

<pre>
indexes = torch.LongTensor(<strong>{{1,1,1,1}}</strong>)
print("Indexes:")
print(indexes)
print("Result:")
print(m1:gather(1, indexes))
</pre>

<p>Výsledky:</p>

<pre>
Indexes:	
 1  1  1  1
[torch.LongTensor of size 1x4]
&nbsp;
Result:	
 1  2  3  4
[torch.DoubleTensor of size 1x4]
</pre>

<p>Opačný případ &ndash; výběr prvních prvků z&nbsp;každého řádku matice:</p>

<pre>
indexes = torch.LongTensor(<strong>{{1},{1},{1},{1},{1},{1}}</strong>)
print("Indexes:")
print(indexes)
print("Result:")
print(m1:gather(2, indexes))
</pre>

<p>Výsledky:</p>

<pre>
Indexes:	
 1
 1
 1
 1
 1
 1
[torch.LongTensor of size 6x1]
&nbsp;
Result:	
  1
  5
  9
 13
 17
 21
[torch.DoubleTensor of size 6x1]
</pre>

<p>Výběr prvního, druhého a třetího řádku tabulky:</p>

<pre>
indexes = torch.LongTensor({{1,1,1,1}, {2,2,2,2}, {3,3,3,3}})
print("Indexes:")
print(indexes)
print("Result:")
print(m1:gather(1, indexes))
</pre>

<p>Výsledky:</p>

<pre>
Indexes:	
 1  1  1  1
 2  2  2  2
 3  3  3  3
[torch.LongTensor of size 3x4]
&nbsp;
Result:	
  1   2   3   4
  5   6   7   8
  9  10  11  12
[torch.DoubleTensor of size 3x4]
</pre>

<p>Výběr z&nbsp;prvních dvou řádků zdrojové matice, ovšem
napřeskáčku:</p>

<pre>
indexes = torch.LongTensor({{1,2,1,2}, {2,1,2,1}})
print("Indexes:")
print(indexes)
print("Result:")
print(m1:gather(1, indexes))
</pre>

<p>Výsledky:</p>

<pre>
Indexes:	
 1  2  1  2
 2  1  2  1
[torch.LongTensor of size 2x4]
&nbsp;
Result:	
 1  6  3  8
 5  2  7  4
[torch.DoubleTensor of size 2x4]
</pre>

<p>Prohození všech řádků matice:</p>
<pre>
indexes = torch.LongTensor({{6,6,6,6}, {5,5,5,5}, {4,4,4,4}, {3,3,3,3}, {2,2,2,2}, {1,1,1,1}})
print("Indexes:")
print(indexes)
print("Result:")
print(m1:gather(1, indexes))
</pre>

<p>Výsledky:</p>

<pre>
Indexes:	
 6  6  6  6
 5  5  5  5
 4  4  4  4
 3  3  3  3
 2  2  2  2
 1  1  1  1
[torch.LongTensor of size 6x4]
&nbsp;
Result:	
 21  22  23  24
 17  18  19  20
 13  14  15  16
  9  10  11  12
  5   6   7   8
  1   2   3   4
[torch.DoubleTensor of size 6x4]
</pre>

<p>Získání matice se dvěma sloupci, prvky jsou získány postupně
z&nbsp;prvního, druhého, třetího... sloupce původní matice:</p>

<pre>
indexes = torch.LongTensor({{1,1}, {2,2}, {3,3}, {4,4}, {1,1}, {1,1}})
print("Indexes:")
print(indexes)
print("Result:")
print(m1:gather(2, indexes))
</pre>

<p>Výsledky:</p>

<pre>
Indexes:	
 1  1
 2  2
 3  3
 4  4
 1  1
 1  1
[torch.LongTensor of size 6x2]
&nbsp;
Result:	
  1   1
  6   6
 11  11
 16  16
 17  17
 21  21
[torch.DoubleTensor of size 6x2]
</pre>

<p>Výběr prvního a čtvrtého sloupce, opět na přeskáčku:</p>

<pre>
indexes = torch.LongTensor({{1,4}, {1,4}, {1,4}, {4,1}, {4,1}, {4,1}})
print("Indexes:")
print(indexes)
print("Result:")
print(m1:gather(2, indexes))
</pre>

<p>Výsledky:</p>

<pre>
Indexes:	
 1  4
 1  4
 1  4
 4  1
 4  1
 4  1
[torch.LongTensor of size 6x2]
&nbsp;
Result:	
  1   4
  5   8
  9  12
 16  13
 20  17
 24  21
[torch.DoubleTensor of size 6x2]
</pre>

<p>Vidíme, že tato operace je skutečně velmi flexibilní.</p>



<p><a name="k07"></a></p>
<h2 id="k07">7. Operace nad vektory a maticemi, násobení vektorů, lineární interpolace a další operace z&nbsp;BLAS</h2>

<p>V&nbsp;knihovně <i>Torch</i> nalezneme mnoho funkcí, které mohou provádět
různé operace nad vektory, maticemi, popř.&nbsp;nad obecnými tenzory. Některé
funkce lze použít pro všechny typy tenzorů (vynásobení komponent tenzoru se
skalární hodnotou), jiné je však možné aplikovat jen na tenzory s&nbsp;jednou
dimenzí (vektorové operace), se dvěma dimenzemi (maticové násobení) či dokonce
jen na tenzory s&nbsp;omezeným počtem komponent (vektorový součin).
V&nbsp;dalších kapitolách si popíšeme následující operace:</p>

<table>
<tr><th>Operace</th><th>Zápis</th><th>Popis v&nbsp;kapitole</th></tr>
<tr><td>Součin tenzoru a skaláru</td><td>torch.mul(tenzor, skalár), tenzor:mul(skalár)</td><td><a href="#k08">8</a></td></tr>
<tr><td>Součin odpovídajících si komponent</td><td>torch.cmul(tenzor1, tenzor2), tenzor2:cmul(tenzor2)</td><td><a href="#k09">9</a></td></tr>
<tr><td>Skalární součin</td><td>torch.dot(vektor1, vektor2), vektor1:dot(vektor2)</td><td><a href="#k10">10</a></td></tr>
<tr><td>Vektorový součin</td><td>torch.cross(vektor1, vektor2), vektor1:cross(vektor2)</td><td><a href="#k11">11</a></td></tr>
<tr><td>Vnější součin</td><td>gtorch.ger(vektor1, vektor2), vektor1:ger(vektor2)</td><td><a href="#k12">12</a></td></tr>
<tr><td>Součin matice a vektoru</td><td>torch.mv(matice, vektor), *</td><td><a href="#k13">13</a></td></tr>
<tr><td>Součin dvou matic</td><td>torch.mm(matice1, matice2), *</td><td><a href="#k14">14</a></td></tr>
</table>

<p>Ze zápisů ve druhém sloupci je patrné, že některé operace jsou dostupné jako
běžné funkce i jako metody. Součin matice a vektoru popř.&nbsp;maticový součin
je možné zapsat i pomocí přetíženého operátoru *.</p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Vynásobení komponent tenzoru (vektoru, matice) skalární hodnotou</h2>

<p>Jednou z&nbsp;nejjednodušších operací, v&nbsp;nichž se vyskytuje součin, je
operace sloužící k&nbsp;vynásobení všech komponent vybraného tenzoru (typicky
vektoru nebo matice) skalární hodnotou. Tato operace je představována metodou
<strong>mul</strong>, kterou lze volat dvěma způsoby:</p>

<pre>
v3 = torch.mul(v1, 10)
v4 = v2:mul(10)
</pre>

<p>V&nbsp;prvním případě se vektor (matice, tenzor) <strong>v1</strong> nezmění
a výsledkem operace je nový tenzor, ovšem v&nbsp;případě druhém dojde
k&nbsp;modifikaci komponent původního tenzoru <strong>v2</strong>. Záleží jen
na rozhodnutí uživatele, zda potřebuje zachovat původní tenzor či zda mu nevadí
provedení součinu s&nbsp;modifikací původního tenzoru (což je samozřejmě časově
i paměťově méně náročné).</p>

<p>Podívejme se na příklad, v&nbsp;němž budeme násobit prvky vektoru [1, 2, 3]
konstantou 10:</p>

<pre>
v1 = torch.Tensor({1,2,3})
v2 = torch.Tensor({1,2,3})
&nbsp;
print("Original vector 1")
print(v1)
&nbsp;
print("Original vector 2")
print(v2)
&nbsp;
v3 = torch.mul(v1, 10)
v4 = v2:mul(10)
&nbsp;
print("Results of vector*scalar")
print(v3)
print(v4)
&nbsp;
print("New value of vector 1")
print(v1)
&nbsp;
print("New value of vector 2")
print(v2)
</pre>

<p>Obsah původních vektorů:</p>

<pre>
Original vector 1	
 1
 2
 3
[torch.DoubleTensor of size 3]
&nbsp;
Original vector 2	
 1
 2
 3
[torch.DoubleTensor of size 3]
</pre>

<p>Výsledek součinu všech prvků vektorů skalární hodnotou:</p>

<pre>
Results of vector*scalar	
 10
 20
 30
[torch.DoubleTensor of size 3]
&nbsp;
 10
 20
 30
[torch.DoubleTensor of size 3]
</pre>

<p>Nový obsah původních vektorů (vidíme, že druhý vektor byl skutečně
modifikován):</p>

<pre>
New value of vector 1	
 1
 2
 3
[torch.DoubleTensor of size 3]
&nbsp;
New value of vector 2	
 10
 20
 30
[torch.DoubleTensor of size 3]
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Vynásobení odpovídajících si komponent tenzoru</h2>

<p>Další operace, v&nbsp;níž se provádí součin, spočívá ve vynásobení
odpovídajících si komponent tenzoru (tj.&nbsp;v&nbsp;praxi opět vektorů či
matic). Tato operace se jmenuje <strong>cmul</strong> a chová se podobně jako
operace <strong>mul</strong> <a href="#k08">z&nbsp;předchozí kapitoly</a>.
První způsob volání nemění ani jeden ze vstupních vektorů, druhý způsob volání
(metody) modifikuje prvky původního vektoru:</p>

<pre>
v3 = torch.cmul(v1, v2)
v4 = v1:cmul(v2)
</pre>

<p>Opět si vše ukažme na jednoduchém příkladu:</p>

<pre>
v1 = torch.Tensor({1,2,3})
v2 = torch.Tensor({1,2,3})
&nbsp;
print("Original vector 1")
print(v1)
&nbsp;
print("Original vector 2")
print(v2)
&nbsp;
v3 = torch.cmul(v1, v2)
v4 = v1:cmul(v2)
&nbsp;
print("Results of vector*vector element-wise")
print(v3)
print(v4)
&nbsp;
print("New value of vector 1")
print(v1)
&nbsp;
print("New value of vector 2")
print(v2)
</pre>

<p>Obsah původních vektorů:</p>

<pre>
Original vector 1	
 1
 2
 3
[torch.DoubleTensor of size 3]
&nbsp;
Original vector 2	
 1
 2
 3
[torch.DoubleTensor of size 3]
</pre>

<p>Výsledek součinu prvek po prvku:</p>

<pre>
Results of vector*vector element-wise	
 1
 4
 9
[torch.DoubleTensor of size 3]
&nbsp;
 1
 4
 9
[torch.DoubleTensor of size 3]
</pre>

<p>Nový obsah původních vektorů (vidíme, že druhý vektor byl skutečně
modifikován):</p>

<pre>
New value of vector 1	
 1
 4
 9
[torch.DoubleTensor of size 3]
&nbsp;
New value of vector 2	
 1
 2
 3
[torch.DoubleTensor of size 3]
</pre>



<p><a name="k10"></a></p>
<h2 id="k10">10. Skalární součin vektorů</h2>

<p>Další podporovanou operací je klasický skalární součin dvou vektorů neboli
anglicky <i>dot product</i>. Tuto operaci je možné zavolat buď pomocí metody
<strong>dot</strong>, nebo alternativně přetíženým operátorem *. Tato operace
NEmodifikuje obsah původních vektorů, takže následující tři příkazy provádí
prakticky stejný výpočet:</p>

<pre>
v3 = torch.dot(v1, v2)
v4 = v1 * v2
v5 = v1:dot(v2)
</pre>

<p>Opět si vše ukážeme na příkladu:</p>

<pre>
v1 = torch.range(1,5)
v2 = torch.range(1,5)
&nbsp;
print("Original vector 1")
print(v1)
&nbsp;
print("Original vector 2")
print(v2)
&nbsp;
v3 = torch.dot(v1, v2)
v4 = v1 * v2
v5 = v1:dot(v2)
&nbsp;
print("Dot products")
print(v3)
print(v4)
print(v5)
&nbsp;
print("New value of vector 1")
print(v1)
&nbsp;
print("New value of vector 2")
print(v2)
</pre>

<p>Původní vektory mají pět prvků:</p>

<pre>
Original vector 1	
 1
 2
 3
 4
 5
[torch.DoubleTensor of size 5]
&nbsp;
Original vector 2	
 1
 2
 3
 4
 5
[torch.DoubleTensor of size 5]
</pre>

<p>Výsledky skalárního součinu jsou ve všech třech případech shodné:</p>

<pre>
Dot products	
55	
55	
55	
</pre>

<p>Ani jeden vektor se přitom nezměnil:</p>

<pre>
New value of vector 1	
 1
 2
 3
 4
 5
[torch.DoubleTensor of size 5]
&nbsp;
New value of vector 2	
 1
 2
 3
 4
 5
[torch.DoubleTensor of size 5]
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. Vektorový součin</h2>

<p>V&nbsp;knihovně Torch je samozřejmě podporována i operace <a
href="https://cs.wikipedia.org/wiki/Vektorov%C3%BD_sou%C4%8Din">vektorového
součinu</a> (<i>cross product</i>). Tato operace předpokládá (a taktéž
kontroluje), jestli mají oba vstupní vektory tři prvky, protože vektorový
součin je definován pro vektory v&nbsp;3D prostoru (výsledkem je vektor kolmý
na oba vstupní vektory). Ukažme si jednoduchý příklad, nejdříve s&nbsp;vektory
[1, 0, 1] a [0, 1, 0] pro ověření, zda je výsledkem opravdu kolmý vektor a
taktéž pro vektory [1, 2, 3] a [3, 2, 1]. Výsledky si můžete ověřit například
<a
href="http://onlinemschool.com/math/assistance/vector/multiply1/">zde</a>:</p>

<pre>
v1 = torch.Tensor({1,0,0})
v2 = torch.Tensor({0,1,0})
&nbsp;
print("Original vector 1")
print(v1)
&nbsp;
print("Original vector 2")
print(v2)
&nbsp;
v3 = torch.cross(v1, v2)
v4 = v1:cross(v2)
&nbsp;
print("Cross products")
print(v3)
print(v4)
&nbsp;
print("New value of vector 1")
print(v1)
&nbsp;
print("New value of vector 2")
print(v2)
&nbsp;
print("----------------------------")
&nbsp;
v1 = torch.Tensor({1,2,3})
v2 = torch.Tensor({3,2,1})
&nbsp;
print("Original vector 1")
print(v1)
&nbsp;
print("Original vector 2")
print(v2)
&nbsp;
v3 = torch.cross(v1, v2)
v4 = v1:cross(v2)
&nbsp;
print("Cross products")
print(v3)
print(v4)
&nbsp;
print("New value of vector 1")
print(v1)
&nbsp;
print("New value of vector 2")
print(v2)
</pre>

<p>Ověření vektorového součinu [1, 0, 0] &times; [0, 1, 0]:</p>

<pre>
Original vector 1	
 1
 0
 0
[torch.DoubleTensor of size 3]
&nbsp;
Original vector 2	
 0
 1
 0
[torch.DoubleTensor of size 3]
&nbsp;
Cross products	
 0
 0
 1
[torch.DoubleTensor of size 3]
&nbsp;
 0
 0
 1
[torch.DoubleTensor of size 3]
&nbsp;
New value of vector 1	
 1
 0
 0
[torch.DoubleTensor of size 3]
&nbsp;
New value of vector 2	
 0
 1
 0
[torch.DoubleTensor of size 3]
</pre>

<p>Ověření vektorového součinu [1, 2, 3] &times; [3, 2, 1]:</p>

<pre>
Original vector 1	
 1
 2
 3
[torch.DoubleTensor of size 3]
&nbsp;
Original vector 2	
 3
 2
 1
[torch.DoubleTensor of size 3]
&nbsp;
Cross products	
-4
 8
-4
[torch.DoubleTensor of size 3]
&nbsp;
-4
 8
-4
[torch.DoubleTensor of size 3]
&nbsp;
New value of vector 1	
 1
 2
 3
[torch.DoubleTensor of size 3]
&nbsp;
New value of vector 2	
 3
 2
 1
[torch.DoubleTensor of size 3]
</pre>



<p><a name="k12"></a></p>
<h2 id="k12">12. Vnější součin</h2>

<p>Další operace, kterou lze provádět s&nbsp;vektory, je vnější součin (<a
href="https://en.wikipedia.org/wiki/Outer_product">outer product</a>).
Vstupními operandy je dvojice vektorů, které mohou mít libovolnou a rozdílnou
délku, výsledkem je dvourozměrná matice. Ve skutečnosti se totiž provádí
maticové násobení prvního vstupního vektoru s&nbsp;transponovaným druhým
vektorem:</p>

<p>
m = v<sub>1</sub>v<sub>2</sub><sup>T</sup>
</p>

<p>Poznámka: pokud naopak budeme transponovat první vstupní vektor, bude
výsledkem maticového násobení jediné číslo odpovídající skalárnímu součinu:</p>

<p>
s = v<sub>1</sub><sup>T</sup>v<sub>2</sub>
</p>

<p>Poznámka: vektory v<sub>1</sub> a v<sub>2</sub> jsou považovány za sloupcové
vektory, proto je zdánlivě transponován vždy opačný vektor, než nám napovídá
intuice.</p>

<p>Vyzkoušejme si nyní provést vnější součin nad dvojicí vektorů [1, 0, 0], [0,
1, 0], dále pak nad dvojicí [1, 2, 4], [1, 2, 3] a konečně nad vektory [1, ...
10] a [1, 2, 3]:</p>

<pre>
v1 = torch.Tensor({1,0,0})
v2 = torch.Tensor({0,1,0})
&nbsp;
print("Original vector 1")
print(v1)
&nbsp;
print("Original vector 2")
print(v2)
&nbsp;
v3 = torch.ger(v1, v2)
&nbsp;
print("Outer products")
print(v3)
&nbsp;
print("---------------")
&nbsp;
v1 = torch.Tensor({1,2,3})
v2 = torch.Tensor({1,2,3})
&nbsp;
print("Original vector 1")
print(v1)
&nbsp;
print("Original vector 2")
print(v2)
&nbsp;
v3 = torch.ger(v1, v2)
&nbsp;
print("Outer products")
print(v3)
&nbsp;
print("---------------")
&nbsp;
v1 = torch.range(1,10)
v2 = torch.Tensor({1,2,3})
&nbsp;
print("Original vector 1")
print(v1)
&nbsp;
print("Original vector 2")
print(v2)
&nbsp;
v3 = torch.ger(v1, v2)
&nbsp;
print("Outer products")
print(v3)
</pre>

<p>Ověření vnějšího součinu [1, 0, 0] &otimes; [0, 1, 0]:</p>

<pre>
Original vector 1	
 1
 0
 0
[torch.DoubleTensor of size 3]
&nbsp;
Original vector 2	
 0
 1
 0
[torch.DoubleTensor of size 3]
&nbsp;
Outer products	
 0  1  0
 0  0  0
 0  0  0
[torch.DoubleTensor of size 3x3]
</pre>

<p>Ověření vnějšího součinu [1, 2, 3] &otimes; [1, 2, 3]:</p>

<pre>
Original vector 1	
 1
 2
 3
[torch.DoubleTensor of size 3]
&nbsp;
Original vector 2	
 1
 2
 3
[torch.DoubleTensor of size 3]
&nbsp;
Outer products	
 1  2  3
 2  4  6
 3  6  9
[torch.DoubleTensor of size 3x3]
</pre>

<p>První sloupec odpovídá prvkům původního vektoru, druhý sloupec nese hodnoty
vynásobené dvěma, třetí pak vynásobené třemi.</p>

<p>Ověření vnějšího součinu [1, 2, 3 ... 10] &otimes; [1, 2, 3]:</p>

<pre>
Original vector 1	
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
[torch.DoubleTensor of size 10]
&nbsp;
Original vector 2	
 1
 2
 3
[torch.DoubleTensor of size 3]
&nbsp;
Outer products	
  1   2   3
  2   4   6
  3   6   9
  4   8  12
  5  10  15
  6  12  18
  7  14  21
  8  16  24
  9  18  27
 10  20  30
[torch.DoubleTensor of size 10x3]
</pre>

<p>Na posledním příkladu je jasně patrné, jak je maticové násobení prováděno
(jen se nenechte zmást tím, že jednodimenzionální tenzory jsou zobrazeny jako
sloupce čísel a nikoli jako řádky).</p>



<p><a name="k13"></a></p>
<h2 id="k13">13. Součin matice a vektoru</h2>

<p>Další podporovanou operací je součin matice a vektoru. Ten je možné zapsat
buď pomocí funkce <strong>torch.mv</strong> nebo s&nbsp;využitím přetíženého
operátoru *. Počet sloupců matice musí odpovídat počtu prvků vektoru. Výsledkem
je vektor s&nbsp;počtem prvků odpovídajícím počtu řádků matice:</p>

<pre>
m = torch.Tensor({{1, 2},
                  {3, 4},
                  {5, 6},
                  {7, 8}})
&nbsp;
v = torch.Tensor({1, 2})
&nbsp;
print("Original matrix")
print(m)
&nbsp;
print("Original vector")
print(v)
&nbsp;
m2 = torch.mv(m, v)
m3 = m * v
&nbsp;
print("New matrix")
print(m2)
print(m3)
</pre>

<p>Původní matice a vektor vstupující do operace:</p>

<pre>
Original matrix	
 1  2
 3  4
 5  6
 7  8
[torch.DoubleTensor of size 4x2]
&nbsp;
Original vector	
 1
 2
[torch.DoubleTensor of size 2]
</pre>

<p>Výsledek operace je shodný při použití <strong>torch.mv</strong> i operátoru
*. Poslední prvek vznikl výpočtem 7*1 + 8*2:</p>

<pre>
New matrix	
  5
 11
 17
 23
[torch.DoubleTensor of size 4]
&nbsp;
  5
 11
 17
 23
[torch.DoubleTensor of size 4]
</pre>



<p><a name="k14"></a></p>
<h2 id="k14">14. Součin dvou matic</h2>

<p>Pochopitelně následuje operace pro součin dvou matic, která se zapisuje buď
funkcí <strong>torch.mm</strong> nebo s&nbsp;využitím přetíženého operátoru *.
U této operace je nutné zajistit, aby počet sloupců první matice odpovídal
počtu řádků matice druhé. Pokud má první matice rozměry m&times;n (m=počet
řádků, n=počet sloupců) a druhá rozměry n&times;p, bude mít výsledná matice
rozměry m&times;p:</p>

<pre>
m1 = torch.Tensor({{1, 2, 3},
                   {4, 5, 6},
                   {7, 8, 9}})
&nbsp;
m2 = torch.eye(3,3)
&nbsp;
m3 = torch.eye(3,3)
m3[{2 ,2}] = 10
&nbsp;
print("Original matrix 1")
print(m1)
&nbsp;
print("Original matrix 2")
print(m2)
&nbsp;
print("Original matrix 3")
print(m3)
&nbsp;
m12 = m1 * m2
&nbsp;
print("m1 x m2:")
print(m12)
&nbsp;
m13 = m1 * m3
&nbsp;
print("m1 x m3:")
print(m13)
</pre>

<p>Původní trojice matic:</p>

<pre>
Original matrix 1	
 1  2  3
 4  5  6
 7  8  9
[torch.DoubleTensor of size 3x3]
&nbsp;
Original matrix 2	
 1  0  0
 0  1  0
 0  0  1
[torch.DoubleTensor of size 3x3]
&nbsp;
Original matrix 3	
  1   0   0
  0  10   0
  0   0   1
[torch.DoubleTensor of size 3x3]
</pre>

<p>Výsledek operace m1·m2:</p>

<pre>
m1 · m2:	
 1  2  3
 4  5  6
 7  8  9
[torch.DoubleTensor of size 3x3]
</pre>

<p>Výsledek operace m1·m3:</p>

<pre>
m1 · m3:	
  1  20   3
  4  50   6
  7  80   9
[torch.DoubleTensor of size 3x3]
</pre>



<p><a name="k15"></a></p>
<h2 id="k15">15. Lineární transformace</h2>

<p>Díky možnosti provést vynásobení vektoru s&nbsp;maticí je možné použít
přetížený operátor * i pro aplikaci lineárních transformací, a to jak
v&nbsp;ploše, tak i v&nbsp;trojrozměrném prostoru. Mezi lineární transformace
patří posun, otáčení, změna měřítka a zkosení. Jestliže máme tři body, které
leží na jedné přímce, tak po aplikaci libovolné lineární transformace získáme
nové tři body, které také leží na přímce (proto jsou transformace lineární).
Všechny lineární transformace lze vyjádřit maticí o rozměrech <i>n &times;
n</i>, kde <i>n</i> je dimenze vektoru či bodu, který transformujeme.</p>

<p>Pokud provádíme lineární transformace v&nbsp;ploše, vyjadřujeme souřadnici
dvěma složkami <i>x</i> a <i>y</i>. Lineární transformaci lze zapsat jako
výpočet nových souřadnic <i>x'</i> a <i>y'</i>:</p>

<p>
x' = xa<sub>11</sub> + ya<sub>12</sub>,<br />
y' = xa<sub>21</sub> + ya<sub>22</sub>.
</p>

<p>Členy a<sub>11</sub>, a<sub>12</sub>, a<sub>21</sub> a a<sub>22</sub>
představují parametry lineární transformace.</p>

<p>Přehlednější je však vektorově/maticový zápis, kdy dvojici <i>[x, y]</i> označíme jako <i>v</i>, dvojici <i>[x', y']</i> jako <i>v'</i> a hodnoty a<sub>11</sub>, a<sub>12</sub>, a<sub>21</sub> a a<sub>22</sub> zapíšeme jako prvky do matice M o rozměrech 2&times;2. Potom lze celou lineární transformaci zapsat jedním vztahem:</p>

<p>
v' = M * v – v tomto případě jsou <i>v</i> a <i>v'</i> sloupcové vektory.</p>
</p>

<p>Pro zadání lineárních transformací v&nbsp;trojrozměrném prostoru by nám měla
stačit transformační matice o rozměrech 3&times;3 prvky. To je skutečně pravda,
ale v&nbsp;počítačové grafice často potřebujeme kromě lineárních transformací
vyjádřit i perspektivní projekci. Proto se ke třem souřadnicím <i>[x, y, z]</i>
přidává ještě čtvrtá souřadnice označovaná <i>w</i> ze slova <i>weight</i>
&ndash; váha. Bod je potom vyjádřen čtveřicí <i>[x, y, z, 1]</i> a vektor
<i>[x, y, z, 0]</i>. Transformační matice je poté zvětšena na rozměry 4&times;4
prvky. Stejně tak pro 2D plochu rozšíříme vektor na <i>[x, y, z]</i> a
transformační matice bude mít 3&times;3 prvky.</p>



<p><a name="k16"></a></p>
<h2 id="k16">16. Příklad aplikace lineární transformace: otáčení bodu či vektoru v&nbsp;ploše</h2>

<p>Ukažme si nyní aplikaci lineární transformace pro otáčení bodu či vektoru
v&nbsp;ploše. Matice otáčení bude mít rozměry 3&times;3 prvky, přičemž poslední
sloupec musí obsahovat hodnoty 0, 0, 1 (neprovádí se nelineární perspektivní
projekce) a spodní řádek bude mít taktéž hodnoty 0, 0, 1 (neprovádí se posun).
Zbývá nám určit prvky podmatice 2&times;2 prvky. Pro matici otáčení tyto prvky
mají následující obsah:</p>

<pre>
| cos &alpha;  -sin &alpha; |
| sin &alpha;   cos &alpha; |
</pre>

<p>Postupné otáčení bodu [1, 0] o 30&deg; (&pi;/6) lze naprogramovat
následovně:</p>

<pre>
trans_identity = torch.eye(3, 3)
&nbsp;
angle = math.pi/6
&nbsp;
trans_rotation = torch.Tensor({{math.cos(angle), -math.sin(angle), 0},
                               {math.sin(angle),  math.cos(angle), 0},
                               {0,                0,               1}})
&nbsp;
v = torch.Tensor({1, 0, 1})
&nbsp;
print("Original point")
print(v)
&nbsp;
v = trans_identity * v
&nbsp;
print("After transformation #1")
print(v)
&nbsp;
v = trans_rotation * v
&nbsp;
print("First rotation by 30")
print(v)
&nbsp;
v = trans_rotation * v
&nbsp;
print("Second rotation by 30°")
print(v)
&nbsp;
print("Third rotation by 30°")
v = trans_rotation * v
&nbsp;
print(v)
</pre>

<p>Výsledky (včetně původního bodu):</p>

<pre>
Original point
 1
 0
 1
[torch.DoubleTensor of size 3]
&nbsp;
After transformation #1	
 1
 0
 1
[torch.DoubleTensor of size 3]
&nbsp;
First rotation by 30	
 0.8660
 0.5000
 1.0000
[torch.DoubleTensor of size 3]
&nbsp;
Second rotation by 30°	
 0.5000
 0.8660
 1.0000
[torch.DoubleTensor of size 3]
&nbsp;
Third rotation by 30°	
 2.7756e-16
 1.0000e+00
 1.0000e+00
[torch.DoubleTensor of size 3]
</pre>

<p>Povšimněte si, že bod se skutečně otočil až na souřadnice [0, 1] (ona nula
je kvůli kumulativní chybě vyjádřena jako 2,7&times;10<sup>-16</sup>).</p>



<p><a name="k17"></a></p>
<h2 id="k17">17. Lineární interpolace mezi dvěma tenzory</h2>

<p>Poslední operací, kterou si dnes podrobněji popíšeme, je operace pro
provedení lineární interpolace mezi komponentami dvou tenzorů. Typicky se jedná
o jednorozměrné vektory či o dvourozměrné matice, ovšem teoreticky není počet
dimenzí nijak omezený. Operace pro výpočet nového tenzoru pomocí lineární
interpolace je představována metodou nazvanou <strong>lerp</strong>, což je
jméno používané například v&nbsp;počítačové grafice (mj.&nbsp;i proto, že se
jedná o jednu ze základních operací prováděných na grafických akcelerátorech).
Oba tenzory předávané metodě <strong>lerp</strong> by měly mít stejný tvar,
tj.&nbsp;shodný počet dimenzí i shodnou velikost. Třetím parametrem této metody
je váha, která je typicky představována reálným číslem ležícím mezi 0,0 až 1,0.
Ve skutečnosti však může být váha jakákoli &ndash; záporné číslo i kladné číslo
větší než nula (musíte však umět interpretovat výsledky).</p>

<p>Podívejme se nyní na způsob vytvoření nového vektoru s&nbsp;pěti prvky
s&nbsp;využitím lineární interpolace mezi vektorem [100, 0, 0, 0, 0] a vektorem
[0, 1, 2, 3, 4]. Váha se postupně mění od nuly do jedničky s&nbsp;krokem
0,2:</p>

<pre>
v1 = torch.Tensor({100, 0, 0, 0, 0})
v2 = torch.range(0, 4)
&nbsp;
print("Source vector 1")
print(v1)
&nbsp;
print("Source vector 2")
print(v2)
&nbsp;
for weight = 0.0, 1.0, 0.2 do
    print("Linear interpolation with weight=" .. weight)
    vt = <strong>torch.lerp(v1, v2, weight)</strong>
    print(vt)
end
</pre>

<p>Zdrojové vektory:</p>

<pre>
Source vector 1	
 100
   0
   0
   0
   0
[torch.DoubleTensor of size 5]
&nbsp;
Source vector 2	
 0
 1
 2
 3
 4
[torch.DoubleTensor of size 5]
</pre>

<p>Pokud je váha nastavená na nulu, vrátí se pochopitelně vektor s&nbsp;prvky
odpovídajícími prvnímu vektoru:</p>

<pre>
Linear interpolation with weight=0	
 100
   0
   0
   0
   0
[torch.DoubleTensor of size 5]
</pre>

<p>Jak se váha postupně zvyšuje, mění se i hodnota prvků výsledného
vektoru:</p>

<pre>
Linear interpolation with weight=0.2	
 80.0000
  0.2000
  0.4000
  0.6000
  0.8000
[torch.DoubleTensor of size 5]
&nbsp;
Linear interpolation with weight=0.4	
 60.0000
  0.4000
  0.8000
  1.2000
  1.6000
[torch.DoubleTensor of size 5]
&nbsp;
Linear interpolation with weight=0.6	
 40.0000
  0.6000
  1.2000
  1.8000
  2.4000
[torch.DoubleTensor of size 5]
&nbsp;
Linear interpolation with weight=0.8	
 20.0000
  0.8000
  1.6000
  2.4000
  3.2000
[torch.DoubleTensor of size 5]
</pre>

<p>Na konci smyčky, kdy váha dosáhne hodnoty 1, získáme prvky odpovídající
druhému vektoru:</p>

<pre>
Linear interpolation with weight=1	
 0
 1
 2
 3
 4
[torch.DoubleTensor of size 5]
</pre>

<p>Stejnou operaci můžeme provést i pro 2D matice, pokud je jejich rozměr a
tvar shodný:</p>

<pre>
m1 = torch.zeros(5, 5)
m2 = torch.range(0, 25):resize(5,5)
&nbsp;
print("Source matrix 1")
print(m1)
&nbsp;
print("Source matrix 2")
print(m2)
&nbsp;
for weight = 0.0, 1.0, 0.2 do
    print("Linear interpolation with weight=" .. weight)
    mt = torch.lerp(m1, m2, weight)
    print(mt)
end
</pre>

<p>Obě zdrojové matice:</p>

<pre>
Source matrix 1	
 0  0  0  0  0
 0  0  0  0  0
 0  0  0  0  0
 0  0  0  0  0
 0  0  0  0  0
[torch.DoubleTensor of size 5x5]
&nbsp;
Source matrix 2	
  0   1   2   3   4
  5   6   7   8   9
 10  11  12  13  14
 15  16  17  18  19
 20  21  22  23  24
[torch.DoubleTensor of size 5x5]
</pre>

<p>Výsledky pro váhu mezi 0 až 1:</p>

<pre>
Linear interpolation with weight=0	
 0  0  0  0  0
 0  0  0  0  0
 0  0  0  0  0
 0  0  0  0  0
 0  0  0  0  0
[torch.DoubleTensor of size 5x5]
&nbsp;
Linear interpolation with weight=0.2	
 0.0000  0.2000  0.4000  0.6000  0.8000
 1.0000  1.2000  1.4000  1.6000  1.8000
 2.0000  2.2000  2.4000  2.6000  2.8000
 3.0000  3.2000  3.4000  3.6000  3.8000
 4.0000  4.2000  4.4000  4.6000  4.8000
[torch.DoubleTensor of size 5x5]
&nbsp;
Linear interpolation with weight=0.4	
 0.0000  0.4000  0.8000  1.2000  1.6000
 2.0000  2.4000  2.8000  3.2000  3.6000
 4.0000  4.4000  4.8000  5.2000  5.6000
 6.0000  6.4000  6.8000  7.2000  7.6000
 8.0000  8.4000  8.8000  9.2000  9.6000
[torch.DoubleTensor of size 5x5]
&nbsp;
Linear interpolation with weight=0.6	
  0.0000   0.6000   1.2000   1.8000   2.4000
  3.0000   3.6000   4.2000   4.8000   5.4000
  6.0000   6.6000   7.2000   7.8000   8.4000
  9.0000   9.6000  10.2000  10.8000  11.4000
 12.0000  12.6000  13.2000  13.8000  14.4000
[torch.DoubleTensor of size 5x5]
&nbsp;
Linear interpolation with weight=0.8	
  0.0000   0.8000   1.6000   2.4000   3.2000
  4.0000   4.8000   5.6000   6.4000   7.2000
  8.0000   8.8000   9.6000  10.4000  11.2000
 12.0000  12.8000  13.6000  14.4000  15.2000
 16.0000  16.8000  17.6000  18.4000  19.2000
[torch.DoubleTensor of size 5x5]
&nbsp;
Linear interpolation with weight=1	
  0   1   2   3   4
  5   6   7   8   9
 10  11  12  13  14
 15  16  17  18  19
 20  21  22  23  24
[torch.DoubleTensor of size 5x5]
</pre>



<p><a name="k18"></a></p>
<h2 id="k18">18. Další operace dostupné v&nbsp;knihovně BLAS</h2>

<p>Ve frameworku <i>Torch</i> najdeme i mnoho dalších užitečných operací, o
nichž se dnes (alespoň prozatím) zmíníme pouze ve stručnosti:</p>

<table>
<tr><th>Metoda</th><th>Stručný popis</th></tr>
<tr><td>torch.clamp</td><td>ořezání všech komponent tak, aby byly v&nbsp;rozsahu min_value, max_value</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>torch.addcmul</td><td>sekvence operací * a + aplikovaná postupně na všechny prvky tenzorů</td></tr>
<tr><td>torch.addcdiv</td><td>sekvence operací / a + aplikovaná postupně na všechny prvky tenzorů</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>torch.addmv</td><td>kombinace operací <strong>add</strong> a <strong>mv</strong> (viz předchozí kapitoly)</td></tr>
<tr><td>torch.addmm</td><td>kombinace operací <strong>add</strong> a <strong>mm</strong> (viz předchozí kapitoly)</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>torch.min</td><td>vrátí komponentu s&nbsp;nejmenší hodnotou</td></tr>
<tr><td>torch.max</td><td>vrátí komponentu s&nbsp;největší hodnotou</td></tr>
<tr><td>torch.mean</td><td>vrátí průměr vypočtený ze všech komponent</td></tr>
<tr><td>torch.median</td><td>vrátí medián vypočtený ze všech komponent</td></tr>
<tr><td>torch.sum</td><td>vypočte sumu prvků</td></tr>
<tr><td>torch.std</td><td>vypočte standardní odchylku</td></tr>
<tr><td>torch.var</td><td>vypočte rozptyl</td></tr>
<tr><td>torch.mode</td><td>vrátí prvek, který se v&nbsp;tenzoru vyskytuje nejčastěji</td></tr>
<tr><td>torch.sort</td><td>setřídění prvků (lze určit osu atd.)</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>torch.bitand</td><td>bitová operace AND všech komponent tenzoru se zadanou hodnotou (skalárem)</td></tr>
<tr><td>torch.cbitand</td><td>bitový operace AND všech komponent tenzoru s&nbsp;hodnotami přečtenými z&nbsp;dalšího tenzoru</td></tr>
<tr><td>torch.bitor</td><td>bitová operace OR všech komponent tenzoru se zadanou hodnotou (skalárem)</td></tr>
<tr><td>torch.cbitor</td><td>bitový operace OR všech komponent tenzoru s&nbsp;hodnotami přečtenými z&nbsp;dalšího tenzoru</td></tr>
<tr><td>torch.bitxor</td><td>bitová operace XOR všech komponent tenzoru se zadanou hodnotou (skalárem)</td></tr>
<tr><td>torch.cbitxor</td><td>bitový operace XOR všech komponent tenzoru s&nbsp;hodnotami přečtenými z&nbsp;dalšího tenzoru</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>torch.lshift</td><td>bitový posun komponent tenzoru o zadanou hodnotu (skalár)</td></tr>
<tr><td>torch.clshift</td><td>bitový posun komponent tenzoru o hodnoty přečtené z&nbsp;dalšího tenzoru</td></tr>
<tr><td>torch.rshift</td><td>bitový posun komponent tenzoru o zadanou hodnotu (skalár)</td></tr>
<tr><td>torch.crshift</td><td>bitový posun komponent tenzoru o hodnoty přečtené z&nbsp;dalšího tenzoru</td></tr>
</table>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Všechny demonstrační příklady, které jsme si popsali v&nbsp;předchozích
kapitolách, najdete v&nbsp;GIT repositáři dostupném na adrese <a
href="https://github.com/tisnik/torch-examples.git">https://github.com/tisnik/torch-examples.git</a>.
Následují odkazy na zdrojové kódy jednotlivých příkladů:</p>

<table>
<tr><th>Příklad</th><th>Odkaz</th></tr>
<td><td>&nbsp;</td><th>&nbsp;</th></td>
<tr><td>graphs/09_contour.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/graphs/09_contour.lua">https://github.com/tisnik/torch-examples/blob/master/graphs/09_contour.lua</a></td></tr>
<tr><td>graphs/10_contour.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/graphs/10_contour.lua">https://github.com/tisnik/torch-examples/blob/master/graphs/10_contour.lua</a></td></tr>
<tr><td>graphs/11_plot_3d.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/graphs/11_plot_3d.lua">https://github.com/tisnik/torch-examples/blob/master/graphs/11_plot_3d.lua</a></td></tr>
<tr><td>graphs/12_lorenz_attractor.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/graphs/12_lorenz_attractor.lua">https://github.com/tisnik/torch-examples/blob/master/graphs/12_lorenz_attractor.lua</a></td></tr>
<td><td>&nbsp;</td><th>&nbsp;</th></td>
<tr><td>blas/01_mul_by_scalar.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/blas/01_mul_by_scalar.lua">https://github.com/tisnik/torch-examples/blob/master/blas/01_mul_by_scalar.lua</a></td></tr>
<tr><td>blas/02_mul_element_wise.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/blas/02_mul_element_wise.lua">https://github.com/tisnik/torch-examples/blob/master/blas/02_mul_element_wise.lua</a></td></tr>
<tr><td>blas/03_dot_product.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/blas/03_dot_product.lua">https://github.com/tisnik/torch-examples/blob/master/blas/03_dot_product.lua</a></td></tr>
<tr><td>blas/04_cross_product.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/blas/04_cross_product.lua">https://github.com/tisnik/torch-examples/blob/master/blas/04_cross_product.lua</a></td></tr>
<tr><td>blas/05_outer_product.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/blas/05_outer_product.lua">https://github.com/tisnik/torch-examples/blob/master/blas/05_outer_product.lua</a></td></tr>
<tr><td>blas/06_matrix_mul_vector.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/blas/06_matrix_mul_vector.lua">https://github.com/tisnik/torch-examples/blob/master/blas/06_matrix_mul_vector.lua</a></td></tr>
<tr><td>blas/07_matrix_mul.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/blas/07_matrix_mul.lua">https://github.com/tisnik/torch-examples/blob/master/blas/07_matrix_mul.lua</a></td></tr>
<tr><td>blas/08_linear_transformation.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/blas/08_linear_transformation.lua">https://github.com/tisnik/torch-examples/blob/master/blas/08_linear_transformation.lua</a></td></tr>
<tr><td>blas/09_lerp.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/blas/09_lerp.lua">https://github.com/tisnik/torch-examples/blob/master/blas/09_lerp.lua</a></td></tr>
<tr><td>blas/10_matrix_lerp.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/blas/10_matrix_lerp.lua">https://github.com/tisnik/torch-examples/blob/master/blas/10_matrix_lerp.lua</a></td></tr>
<td><td>&nbsp;</td><th>&nbsp;</th></td>
<tr><td>24_gather_from_vector.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/24_gather_from_vector.lua">https://github.com/tisnik/torch-examples/blob/master/basics/24_gather_from_vector.lua</a></td></tr>
<tr><td>25_gather_from_matrix.lua</td><td><a href="https://github.com/tisnik/torch-examples/blob/master/basics/25_gather_from_matrix.lua">https://github.com/tisnik/torch-examples/blob/master/basics/25_gather_from_matrix.lua</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>Stránka projektu Torch<br />
<a href="http://torch.ch/">http://torch.ch/</a>
</li>

<li>Torch: Serialization<br />
<a href="https://github.com/torch/torch7/blob/master/doc/serialization.md">https://github.com/torch/torch7/blob/master/doc/serialization.md</a>
</li>

<li>Torch na GitHubu (několik repositářů)<br />
<a href="https://github.com/torch">https://github.com/torch</a>
</li>

<li>Torch (machine learning), Wikipedia<br />
<a href="https://en.wikipedia.org/wiki/Torch_%28machine_learning%29">https://en.wikipedia.org/wiki/Torch_%28machine_learning%29</a>
</li>

<li>Torch Package Reference Manual<br />
<a href="https://github.com/torch/torch7/blob/master/README.md">https://github.com/torch/torch7/blob/master/README.md</a>
</li>

<li>Torch Cheatsheet<br />
<a href="https://github.com/torch/torch7/wiki/Cheatsheet">https://github.com/torch/torch7/wiki/Cheatsheet</a>
</li>

<li>Lorenzův atraktor<br />
<a href="http://www.root.cz/clanky/fraktaly-v-pocitacove-grafice-iii/#k03">http://www.root.cz/clanky/fraktaly-v-pocitacove-grafice-iii/#k03</a>
</li>

<li>Dot product (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Dot_product">https://en.wikipedia.org/wiki/Dot_product</a>
</li>

<li>Cross product (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Cross_product">https://en.wikipedia.org/wiki/Cross_product</a>
</li>

<li>Outer product<br />
<a href="https://en.wikipedia.org/wiki/Outer_product">https://en.wikipedia.org/wiki/Outer_product</a>
</li>

<li>Linear interpolation<br />
<a href="https://en.wikipedia.org/wiki/Linear_interpolation">https://en.wikipedia.org/wiki/Linear_interpolation</a>
</li>

<li>Matrix multiplication<br />
<a href="https://en.wikipedia.org/wiki/Matrix_multiplication">https://en.wikipedia.org/wiki/Matrix_multiplication</a>
</li>

<li>Demos for gnuplot version 5.2<br />
<a href="http://gnuplot.info/demo/">http://gnuplot.info/demo/</a>
</li>

<li>Plotting with Torch7<br />
<a href="http://www.lighting-torch.com/2015/08/24/plotting-with-torch7/">http://www.lighting-torch.com/2015/08/24/plotting-with-torch7/</a>
</li>

<li>Plotting Package Manual with Gnuplot<br />
<a href="https://github.com/torch/gnuplot/blob/master/README.md">https://github.com/torch/gnuplot/blob/master/README.md</a>
</li>

<li>An Introduction to Tensors<br />
<a href="https://math.stackexchange.com/questions/10282/an-introduction-to-tensors">https://math.stackexchange.com/questions/10282/an-introduction-to-tensors</a>
</li>

<li>Differences between a matrix and a tensor<br />
<a href="https://math.stackexchange.com/questions/412423/differences-between-a-matrix-and-a-tensor">https://math.stackexchange.com/questions/412423/differences-between-a-matrix-and-a-tensor</a>
</li>

<li>Qualitatively, what is the difference between a matrix and a tensor?<br />
<a href="https://math.stackexchange.com/questions/1444412/qualitatively-what-is-the-difference-between-a-matrix-and-a-tensor?">https://math.stackexchange.com/questions/1444412/qualitatively-what-is-the-difference-between-a-matrix-and-a-tensor?</a>
</li>

<li>BLAS (Basic Linear Algebra Subprograms)<br />
<a href="http://www.netlib.org/blas/">http://www.netlib.org/blas/</a>
</li>

<li>Basic Linear Algebra Subprograms (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms</a>
</li>

<li>Helix<br />
<a href="https://en.wikipedia.org/wiki/Helix">https://en.wikipedia.org/wiki/Helix</a>
</li>

<li>Comparison of deep learning software<br />
<a href="https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software">https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software</a>
</li>

<li>TensorFlow<br />
<a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>
</li>

<li>Caffe2 (A New Lightweight, Modular, and Scalable Deep Learning Framework)<br />
<a href="https://caffe2.ai/">https://caffe2.ai/</a>
</li>

<li>PyTorch<br />
<a href="http://pytorch.org/">http://pytorch.org/</a>
</li>

<li>Seriál o programovacím jazyku Lua<br />
<a href="http://www.root.cz/serialy/programovaci-jazyk-lua/">http://www.root.cz/serialy/programovaci-jazyk-lua/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (2)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-2/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-2/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (3)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-3/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-3/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (4)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-4/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-4/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (5 - tabulky a pole)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-5-tabulky-a-pole/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-5-tabulky-a-pole/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (6 - překlad programových smyček do mezijazyka LuaJITu)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-6-preklad-programovych-smycek-do-mezijazyka-luajitu/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-6-preklad-programovych-smycek-do-mezijazyka-luajitu/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (7 - dokončení popisu mezijazyka LuaJITu)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-7-dokonceni-popisu-mezijazyka-luajitu/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-7-dokonceni-popisu-mezijazyka-luajitu/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (8 - základní vlastnosti trasovacího JITu)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-8-zakladni-vlastnosti-trasovaciho-jitu/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-8-zakladni-vlastnosti-trasovaciho-jitu/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (9 - další vlastnosti trasovacího JITu)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-9-dalsi-vlastnosti-trasovaciho-jitu/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-9-dalsi-vlastnosti-trasovaciho-jitu/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (10 - JIT překlad do nativního kódu)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-10-jit-preklad-do-nativniho-kodu/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-10-jit-preklad-do-nativniho-kodu/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (11 - JIT překlad do nativního kódu procesorů s architekturami x86 a ARM)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-11-jit-preklad-do-nativniho-kodu-procesoru-s-architekturami-x86-a-arm/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-11-jit-preklad-do-nativniho-kodu-procesoru-s-architekturami-x86-a-arm/</a>
</li>

<li>LuaJIT - Just in Time překladač pro programovací jazyk Lua (12 - překlad operací s reálnými čísly)<br />
<a href="http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-12-preklad-operaci-s-realnymi-cisly/">http://www.root.cz/clanky/luajit-just-in-time-prekladac-pro-programovaci-jazyk-lua-12-preklad-operaci-s-realnymi-cisly/</a>
</li>

<li>Lua Profiler (GitHub)<br />
<a href="https://github.com/luaforge/luaprofiler">https://github.com/luaforge/luaprofiler</a>
</li>

<li>Lua Profiler (LuaForge)<br />
<a href="http://luaforge.net/projects/luaprofiler/">http://luaforge.net/projects/luaprofiler/</a>
</li>

<li>ctrace<br />
<a href="http://webserver2.tecgraf.puc-rio.br/~lhf/ftp/lua/">http://webserver2.tecgraf.puc-rio.br/~lhf/ftp/lua/</a>
</li>

<li>The Lua VM, on the Web<br />
<a href="https://kripken.github.io/lua.vm.js/lua.vm.js.html">https://kripken.github.io/lua.vm.js/lua.vm.js.html</a>
</li>

<li>Lua.vm.js REPL<br />
<a href="https://kripken.github.io/lua.vm.js/repl.html">https://kripken.github.io/lua.vm.js/repl.html</a>
</li>

<li>lua2js<br />
<a href="https://www.npmjs.com/package/lua2js">https://www.npmjs.com/package/lua2js</a>
</li>

<li>lua2js na GitHubu<br />
<a href="https://github.com/basicer/lua2js-dist">https://github.com/basicer/lua2js-dist</a>
</li>

<li>Lua (programming language)<br />
<a href="http://en.wikipedia.org/wiki/Lua_(programming_language)">http://en.wikipedia.org/wiki/Lua_(programming_language)</a>
</li>

<li>LuaJIT 2.0 SSA IR
<a href="http://wiki.luajit.org/SSA-IR-2.0">http://wiki.luajit.org/SSA-IR-2.0</a>
</li>

<li>The LuaJIT Project<br />
<a href="http://luajit.org/index.html">http://luajit.org/index.html</a>
</li>

<li>LuaJIT FAQ<br />
<a href="http://luajit.org/faq.html">http://luajit.org/faq.html</a>
</li>

<li>LuaJIT Performance Comparison<br />
<a href="http://luajit.org/performance.html">http://luajit.org/performance.html</a>
</li>

<li>LuaJIT 2.0 intellectual property disclosure and research opportunities<br />
<a href="http://article.gmane.org/gmane.comp.lang.lua.general/58908">http://article.gmane.org/gmane.comp.lang.lua.general/58908</a>
</li>

<li>LuaJIT Wiki<br />
<a href="http://wiki.luajit.org/Home">http://wiki.luajit.org/Home</a>
</li>

<li>LuaJIT 2.0 Bytecode Instructions<br />
<a href="http://wiki.luajit.org/Bytecode-2.0">http://wiki.luajit.org/Bytecode-2.0</a>
</li>

<li>Programming in Lua (first edition)<br />
<a href="http://www.lua.org/pil/contents.html">http://www.lua.org/pil/contents.html</a>
</li>

<li>Lua 5.2 sources<br />
<a href="http://www.lua.org/source/5.2/">http://www.lua.org/source/5.2/</a>
</li>

<li>REPL<br />
<a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop</a>
</li>

<li>The LLVM Compiler Infrastructure<br />
<a href="http://llvm.org/ProjectsWithLLVM/">http://llvm.org/ProjectsWithLLVM/</a>
</li>

<li>clang: a C language family frontend for LLVM<br />
<a href="http://clang.llvm.org/">http://clang.llvm.org/</a>
</li>

<li>LLVM Backend ("Fastcomp")<br />
<a href="http://kripken.github.io/emscripten-site/docs/building_from_source/LLVM-Backend.html#llvm-backend">http://kripken.github.io/emscripten-site/docs/building_from_source/LLVM-Backend.html#llvm-backend</a>
</li>

<li>Lambda the Ultimate: Coroutines in Lua,<br />
<a href="http://lambda-the-ultimate.org/node/438">http://lambda-the-ultimate.org/node/438</a>
</li>

<li>Coroutines Tutorial,<br />
<a href="http://lua-users.org/wiki/CoroutinesTutorial">http://lua-users.org/wiki/CoroutinesTutorial</a>
</li>

<li>Lua Coroutines Versus Python Generators,<br />
<a href="http://lua-users.org/wiki/LuaCoroutinesVersusPythonGenerators">http://lua-users.org/wiki/LuaCoroutinesVersusPythonGenerators</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2017</small></p>
</body>
</html>

